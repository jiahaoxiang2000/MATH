\chapter{Vector Spaces and linear mappings}
\label{chap:VectorSpaces}

Broadly speaking, a vector space over a field $F$ refers to a set $V$ together with two operations:
\begin{itemize}
  \item Vector addition $+: V \times V \rightarrow V$, denoted $(v_1, v_2) \mapsto v_1 + v_2$, satisfying associativity, commutativity, and the existence of inverses;
  \item Scalar multiplication $\cdot: F \times V \rightarrow V$, denoted $(t, v) \mapsto t \cdot v = tv$, satisfying associativity and distributivity over addition.
\end{itemize}

If a mapping $T: V \rightarrow W$ between vector spaces satisfies the identities
\begin{align*}
  T(v_1 + v_2) &= T(v_1) + T(v_2),\\
  T(tv) &= tT(v),
\end{align*}
then $T$ is called a linear mapping.

\section{Introduction: Back to the system of linear equations}

\begin{align}
  a_{11}X_1 + \cdots + a_{1n}X_n &= b_1 \nonumber\\
  a_{21}X_1 + \cdots + a_{2n}X_n &= b_2 \nonumber \\
  &\vdots\nonumber \\
  a_{m1}X_1 + \cdots + a_{mn}X_n &= b_m \label{eq:linear-system-2}
\end{align}

\begin{definition}
  Consider a system of $n$ linear equations over a field $F$ in the form (\ref{eq:linear-system-2}). If $b_1 = \cdots = b_m = 0$, then the system is called homogeneous.
\end{definition}

Given $n,m \in \mathbb{Z}_{\geq 1}$ and a family of coefficients $(a_{ij})_{1 \leq i \leq m, 1 \leq j \leq n}$ where $a_{ij} \in F$, define the mapping
\begin{align*}
  T: F^n &\rightarrow F^m\\
  (x_j)_{j=1}^n &\mapsto \left(\sum_{j=1}^n a_{1j}x_j, \ldots, \sum_{j=1}^n a_{mj}x_j\right).
\end{align*}

\begin{definition}
  Let $T: F^n \rightarrow F^m$ correspond to a homogeneous system of linear equations as described above. If $v_1, \ldots, v_h \in F^n$ are all solutions of the system, and every solution $x \in F^n$ can be uniquely expressed through addition and scalar multiplication as
  \begin{align}
    x = \sum_{i=1}^h t_i v_i, \quad t_1, \ldots, t_h \in F,
  \end{align}
  where the tuple $(t_1, \ldots, t_h)$ is uniquely determined by $x$, then $v_1, \ldots, v_h$ is called a fundamental system of solutions for the homogeneous system.
\end{definition}

\begin{proposition}
  Consider a homogeneous system of $n$ linear equations in the form (Eq.\ref{eq:linear-system-2}), where $\bf{b} = 0$. If the reduced row echelon matrix obtained by elimination has $r$ pivot elements, then the corresponding homogeneous system has a fundamental system of solutions $v_1, \ldots, v_{n-r}$.
\end{proposition}

\section{Vector spaces}

\begin{definition}
  A vector space over a field $F$, also called an $F$-vector space, is a tuple $(V, +, \cdot, 0_V)$ where $V$ is a set, $0_V \in V$, and operations $+: V \times V \rightarrow V$ and $\cdot: F \times V \rightarrow V$ are written as $(u,v) \mapsto u + v$ and $(t,v) \mapsto t \cdot v$ respectively, satisfying the following conditions:

  1. Addition satisfies:
  \begin{itemize}
    \item Associativity: $(u + v) + w = u + (v + w)$;
    \item Identity element: $v + 0_V = v = 0_V + v$;
    \item Commutativity: $u + v = v + u$;
    \item Additive inverse: For every $v$, there exists $-v$ such that $v + (-v) = 0_V$.
  \end{itemize}

  2. Scalar multiplication, often written as $tv$ instead of $t \cdot v$, satisfies:
  \begin{itemize}
    \item Associativity: $s \cdot (t \cdot v) = (st) \cdot v$;
    \item Identity property: $1 \cdot v = v$, where $1$ is the multiplicative identity in $F$.
  \end{itemize}

  3. Scalar multiplication distributes over addition:
  \begin{itemize}
    \item First distributive property: $(s + t) \cdot v = s \cdot v + t \cdot v$;
    \item Second distributive property: $s \cdot (u + v) = s \cdot u + s \cdot v$.
  \end{itemize}

  Where $u, v, w$ (or $s, t$) represent arbitrary elements of $V$ (or $F$). When there is no risk of confusion, we denote $0_V$ simply as $0$, write $u + (-v)$ as $u - v$, and refer to the structure $(V, +, \cdot, 0)$ simply as $V$.
\end{definition}

\begin{definition}
  Let $V$ be an $F$-vector space. If a subset $V_0$ of $V$ contains $0$ and is closed under addition and scalar multiplication, then $(V_0, +, \cdot, 0)$ is also an $F$-vector space, called a subspace of $V$.
\end{definition}

\section{Matrix \& calculate}

\begin{definition}
  Let $m,n \in \mathbb{Z}_{\geq 1}$. An $m \times n$ matrix over a field $F$ is a rectangular array
  \begin{align*}
    A = (a_{ij})_{1 \leq i \leq m, 1 \leq j \leq n} =
    \begin{pmatrix}
      a_{11} & \cdots & a_{1n} \\
      \vdots & \ddots & \vdots \\
      a_{m1} & \cdots & a_{mn}
    \end{pmatrix} =
    \begin{bmatrix}
      \cdots & \cdots & a_{ij} & \cdots & \cdots \\
    \end{bmatrix}
  \end{align*}

  \noindent where $a_{ij} \in F$ is called the $(i,j)$-entry or $(i,j)$-element of matrix $A$, with $i$ indicating the row and $j$ indicating the column. An $n \times n$ matrix is called a square matrix of order $n$.

  We denote the set of all $m \times n$ matrices over $F$ by $M_{m \times n}(F)$.
\end{definition}

\begin{proposition}
  The set $M_{m \times n}(F)$ equipped with standard addition and scalar multiplication forms an $F$-vector space. The zero element is the zero matrix, and the additive inverse of a matrix $A = (a_{ij})_{i,j}$ is $-A = (-a_{ij})_{i,j}$.
\end{proposition}

\begin{definition}[Matrix Multiplication]
  Matrix multiplication is a mapping defined as:
  \begin{align*}
    M_{m \times n}(F) \times M_{n \times r}(F) &\rightarrow M_{m \times r}(F)\\
    (A, B) &\mapsto AB
  \end{align*}

  If $A = (a_{ij})_{1 \leq i \leq m, 1 \leq j \leq n}$ and $B = (b_{jk})_{1 \leq j \leq n, 1 \leq k \leq r}$, then $AB = (c_{ik})_{1 \leq i \leq m, 1 \leq k \leq r}$, where:
  \begin{align*}
    c_{ik} := \sum_{j=1}^{n} a_{ij}b_{jk} =
    \begin{pmatrix} a_{i1} & \cdots & a_{in}
    \end{pmatrix}
    \begin{pmatrix} b_{1k} \\ \vdots \\ b_{nk}
    \end{pmatrix}
  \end{align*}

  This represents the dot product of the $i^{th}$ row of $A$ with the $k^{th}$ column of $B$.
\end{definition}

\begin{proposition}
  Matrix multiplication satisfies the following properties:
  \begin{itemize}
    \item Associativity: $(AB)C = A(BC)$;
    \item Distributivity: $A(B + C) = AB + AC$ and $(B + C)A = BA + CA$;
    \item Linearity: $A(tB) = t(AB) = (tA)B$;
  \end{itemize}
  where $t \in F$ and matrices $A, B, C$ are arbitrary, provided their dimensions make these operations valid.
\end{proposition}

\section{Bases \& Dimensions}

\begin{definition}
  Let $S$ be a subset of an $F$-vector space $V$.
  \begin{itemize}
    \item If $\langle S \rangle = V$, then $S$ is said to generate $V$, or $S$ is called a generating set of $V$.
    \item A linear relation in $S$ is an equation of the form
      \begin{align*}
        \sum_{s \in S} a_s s = 0
      \end{align*}
      This relation is called trivial if all coefficients $a_s$ are zero; otherwise, it is non-trivial. The set $S$ is linearly dependent if there exists a non-trivial linear relation among its elements; otherwise, $S$ is linearly independent.
    \item If $S$ is a linearly independent generating set, then $S$ is called a basis of $V$.
  \end{itemize}
\end{definition}

\begin{lemma}
  For any subset $S$ of an $F$-vector space $V$, the following statements are equivalent:
  \begin{enumerate}
    \item $S$ is a minimal generating set.
    \item $S$ is a basis.
    \item $S$ is a maximal linearly independent subset.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Let's prove the equivalence by showing $(1) \Rightarrow (2)$, $(2) \Rightarrow (3)$, and $(3) \Rightarrow (1)$.

  $(1) \Rightarrow (2)$: If $S$ is a minimal generating set, then $\langle S \rangle = V$. Suppose $S$ is not linearly independent. Then there exists some $s_0 \in S$ that can be expressed as a linear combination of other elements in $S$. But this means $S \setminus \{s_0\}$ still generates $V$, contradicting the minimality of $S$. Therefore, $S$ must be linearly independent, making it a basis.

  $(2) \Rightarrow (3)$: Let $S$ be a basis. Then $S$ is linearly independent and $\langle S \rangle = V$. To show that $S$ is maximal, suppose we add any vector $v \not\in S$ to form $S' = S \cup \{v\}$. Since $\langle S \rangle = V$, we have $v \in \langle S \rangle$, meaning $v$ can be written as a linear combination of elements in $S$. Therefore, $S'$ must be linearly dependent, proving that $S$ is a maximal linearly independent set.

  $(3) \Rightarrow (1)$: Let $S$ be a maximal linearly independent set. If $\langle S \rangle \neq V$, then there exists some $v \in V \setminus \langle S \rangle$. The set $S \cup \{v\}$ would still be linearly independent, contradicting the maximality of $S$. Therefore $\langle S \rangle = V$. Now suppose $S$ is not minimal. Then there exists a proper subset $S' \subset S$ with $\langle S' \rangle = V$. But this means some element in $S \setminus S'$ can be expressed as a linear combination of elements in $S'$, making $S$ linearly dependent, which is a contradiction. Thus, $S$ is a minimal generating set.
\end{proof}

\begin{proposition}
  Consider a family of $F$-vector spaces $V_i$, each with a given basis $B_i$, where $i$ ranges over a given index set $I$.
  Embed each $V_i$ as a subspace of the direct sum $\bigoplus_{j \in I} V_j$.
  Correspondingly, view each $B_i$ as a subset of $\bigoplus_{j \in I} V_j$.
  These subsets $B_i$ are pairwise disjoint, and $\bigcup_{i \in I} B_i$ forms a basis for $\bigoplus_{i \in I} V_i$.
\end{proposition}

\begin{definition}
  Every $F$-vector space $V$ has a basis. In fact, any linearly independent subset of $V$ can be extended to a basis.

  Furthermore, all bases of $V$ have the same cardinality. This common cardinality is called the dimension of $V$, denoted by $\dim_F V$ or simply $\dim V$.
\end{definition}

\begin{lemma}
  Let $\{s_1,...,s_n\}$ be a generating set for an $F$-vector space $V$. If $m > n$, then any collection of $m$ vectors $v_1,...,v_m \in V$ is linearly dependent.
\end{lemma}

\section{Linear mappings}

\begin{definition}[Linear Mapping]
  Let $V$ and $W$ be $F$-vector spaces. A mapping $T: V \rightarrow W$ is called a linear mapping (also known as a linear transformation or linear operator) if it satisfies:
  \begin{align*}
    T(v_1 + v_2) &= T(v_1) + T(v_2), & \forall v_1, v_2 \in V,\\
    T(tv) &= tT(v), & \forall t \in F, v \in V.
  \end{align*}
\end{definition}

\begin{lemma}
  If $T: U \rightarrow V$ and $S: V \rightarrow W$ are linear mappings, then their composition $S \circ T: U \rightarrow W$ is also a linear mapping.
\end{lemma}

\begin{proof}
  We need to verify that $S \circ T$ satisfies the two defining properties of linear mappings:

  1. For any $u_1, u_2 \in U$:
  \begin{align*}
    (S \circ T)(u_1 + u_2) &= S(T(u_1 + u_2)) \\
    &= S(T(u_1) + T(u_2)) \quad \text{(by linearity of $T$)} \\
    &= S(T(u_1)) + S(T(u_2)) \quad \text{(by linearity of $S$)} \\
    &= (S \circ T)(u_1) + (S \circ T)(u_2)
  \end{align*}

  2. For any $t \in F$ and $u \in U$:
  \begin{align*}
    (S \circ T)(tu) &= S(T(tu)) \\
    &= S(tT(u)) \quad \text{(by linearity of $T$)} \\
    &= tS(T(u)) \quad \text{(by linearity of $S$)} \\
    &= t(S \circ T)(u)
  \end{align*}

  Therefore, $S \circ T$ is a linear mapping.
\end{proof}

\begin{definition}
  If a linear mapping $T: V \rightarrow W$ is both left and right invertible, it is called an invertible linear mapping or an isomorphism.

  In this case, there exists a unique linear mapping $T^{-1}: W \rightarrow V$ such that $T^{-1} \circ T = \operatorname{id}_V$ and $T \circ T^{-1} = \operatorname{id}_W$. This mapping $T^{-1}$ is called the inverse of $T$; it is simultaneously the unique left inverse and the unique right inverse of $T$.
\end{definition}

\begin{definition}
  Let $V$ and $W$ be $F$-vector spaces. Define $\operatorname{Hom}(V,W)$ as the set of all linear mappings from $V$ to $W$. Addition and scalar multiplication are defined by:
  \begin{align*}
    (T_1 + T_2)(v) &= T_1(v) + T_2(v)\\
    (tT)(v) &= t \cdot T(v)
  \end{align*}

  The zero element of $\operatorname{Hom}(V,W)$ is the zero mapping $0: V \rightarrow W$.
\end{definition}

\section{Linear mappings to Matrix}

\begin{definition}
  Let $V$ be an $F$-vector space. We denote $\operatorname{End}(V) := \operatorname{Hom}(V,V)$, whose elements are called endomorphisms of $V$.
\end{definition}

\begin{corollary}
  Let $V$ be an $F$-vector space. Then $\operatorname{End}(V)$ forms a ring where addition is the addition of linear maps, and multiplication is the composition of linear maps $(S,T) \mapsto ST$. The zero element is the zero mapping, and the multiplicative identity is the identity mapping $\operatorname{id}_V$. Moreover, $\operatorname{End}(V)$ is the zero ring if and only if $V = \{0\}$.
\end{corollary}

\begin{theorem}
  Let $V$ and $W$ be finite-dimensional vector spaces with ordered bases $v_1,\ldots,v_n$ and $w_1,\ldots,w_m$ respectively, where $n,m \in \mathbb{Z}_{\geq 1}$. Then there exists a vector space isomorphism:
  \begin{align*}
    \mathcal{M}: \operatorname{Hom}(V,W) \stackrel{1:1}{\longrightarrow} M_{m\times n}(F)
  \end{align*}
  mapping $T \mapsto (a_{ij})_{1 \leq i \leq m, 1 \leq j \leq n}$, where $(a_{ij})_{i,j} = \mathcal{M}(T)$ is characterized by the property:
  \begin{align*}
    T(v_j) = \sum_{i=1}^m a_{ij}w_i, \quad 1 \leq j \leq n.
  \end{align*}
  Consequently, $\dim \operatorname{Hom}(V,W) = \dim V \cdot \dim W$.
\end{theorem}

\begin{theorem}
  Let $U$, $V$, and $W$ be finite-dimensional vector spaces with ordered bases $u_1,\ldots,u_r$, $v_1,\ldots,v_n$, and $w_1,\ldots,w_m$ respectively. The following diagram commutes:

  \begin{center}
    \begin{tikzcd}
      \operatorname{Hom}(V,W) \times \operatorname{Hom}(U,V) \arrow[r, "\text{Map composition}"] \arrow[d, "M \times M"'] & \operatorname{Hom}(U,W) \arrow[d, "M"] \\
      M_{m \times n}(F) \times M_{n \times r}(F) \arrow[r, "\text{Matrix multiplication}"'] & M_{m \times r}(F)
    \end{tikzcd}
  \end{center}

  In other words, $M(S \circ T) = M(S) \cdot M(T)$, where the left side represents composition of maps and the right side represents matrix multiplication.
\end{theorem}

\begin{definition}
  A matrix $A \in M_{m\times n}(F)$ is called left invertible (or right invertible) if there exists $B \in M_{n\times m}(F)$ such that $BA = 1_{n\times n}$ (or $AB = 1_{m\times m}$). Such a matrix $B$ is called a left inverse (or right inverse) of $A$.

  If $m = n$ and $A$ is both left and right invertible, then $A$ is called an invertible $n\times n$ matrix. In this case, there exists a unique matrix $A^{-1} \in M_{n\times n}(F)$ such that $A^{-1}A = 1_{n\times n} = AA^{-1}$. This matrix $A^{-1}$ serves simultaneously as both the unique left inverse and the unique right inverse of $A$.

  In other words, invertible matrices are precisely the invertible elements in the ring $M_{m\times m}(F)$.
\end{definition}

\section{Transpose of a Matrix and Dual Spaces}

\begin{definition}
  Let $R$ be a ring. For any $A = (a_{ij}) \in M_{m \times n}(R)$, the transpose of $A$, denoted ${}^t\!A$, is the $n \times m$ matrix $(a_{ji})$. That is,
  \[
    A =
    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}
    \implies
    {}^t\!A =
    \begin{pmatrix}
      a_{11} & a_{21} & \cdots & a_{m1} \\
      a_{12} & a_{22} & \cdots & a_{m2} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{1n} & a_{2n} & \cdots & a_{mn}
    \end{pmatrix}
  \]
\end{definition}

\begin{proposition}
  Let $A \in M_{m \times m}(F)$. Then $A$ is invertible if and only if ${}^t\!A$ is invertible. Moreover, in this case,
  \[
    ({}^t\!A)^{-1} = {}^t\!(A^{-1}).
  \]
\end{proposition}

\begin{proof}
  Suppose $A$ is invertible, so $A^{-1}A = AA^{-1} = I$. Taking transposes, $({}^t\!A^{-1})({}^t\!A) = {}^t\!(A A^{-1}) = {}^t\!I = I$, and $({}^t\!A)({}^t\!A^{-1}) = {}^t\!(A^{-1}A) = I$. Thus, ${}^t\!A$ is invertible and $({}^t\!A)^{-1} = {}^t\!(A^{-1})$.

  Conversely, if ${}^t\!A$ is invertible, the same argument applied to ${}^t\!A$ shows $A$ is invertible.
\end{proof}

\begin{definition}
  Let $V$ be a vector space over a field $F$. The dual space of $V$, denoted $V^\vee$, is defined as
  \[
    V^\vee := \operatorname{Hom}(V, F),
  \]
  the set of all linear functionals from $V$ to $F$.
\end{definition}

\begin{definition}
  Let $T: V \to W$ be a linear map between vector spaces over $F$. The transpose (dual) map of $T$, denoted ${}^t\!T : W^\vee \to V^\vee$, is defined by
  \[
    {}^t\!T(\lambda) = \lambda \circ T, \quad \forall\, \lambda \in W^\vee.
  \]
  That is, for any $v \in V$,
  \[
    ({}^t\!T(\lambda))(v) = \lambda(T(v)).
  \]
\end{definition}

\begin{proposition}
  Let $U, V, W$ be vector spaces over $F$, and let $T: U \to V$, $S: V \to W$ be linear maps. Then
  \[
    {}^t\!(S T) = {}^t\!T \circ {}^t\!S \in \operatorname{Hom}(W^\vee, U^\vee).
  \]
\end{proposition}

\begin{proof}
  For any $\lambda \in W^\vee$ and $u \in U$,
  \[
    ({}^t\!(S T)(\lambda))(u) = \lambda((S T)(u)) = \lambda(S(T(u))) = ({}^t\!S(\lambda))(T(u)) = ({}^t\!T({}^t\!S(\lambda)))(u).
  \]
  Thus, ${}^t\!(S T) = {}^t\!T \circ {}^t\!S$.
\end{proof}

\begin{proposition}
  Let $V$ be a finite-dimensional vector space with basis $v_1, \ldots, v_n$. For each $1 \leq i \leq n$, define $\check{v}_i \in V^\vee$ by
  \[
    \check{v}_i\left(\sum_{j=1}^n x_j v_j\right) = x_i.
  \]
  Then $\check{v}_1, \ldots, \check{v}_n$ form a basis of $V^\vee$, called the dual basis of $v_1, \ldots, v_n$.
\end{proposition}

\begin{proof}
  Each $\check{v}_i$ is linear, and for any $v_j$ we have $\check{v}_i(v_j) = \delta_{ij}$. Any $f \in V^\vee$ is determined by its values $f(v_j)$, so $f = \sum_{i=1}^n f(v_i)\check{v}_i$. Thus, $\{\check{v}_1, \ldots, \check{v}_n\}$ is a basis for $V^\vee$.
\end{proof}

\section{Kernel, Image, and Gaussian Elimination}

\begin{definition}
  Let $T: V \to W$ be a linear map between vector spaces. The \textbf{kernel} of $T$ is
  \[
    \ker T := \{ v \in V : T(v) = 0 \} = T^{-1}(0).
  \]
  The \textbf{image} of $T$ is
  \[
    \operatorname{im} T := \{ w \in W : \exists v \in V,\, T(v) = w \}.
  \]
\end{definition}

\begin{proposition}
  Let $T: V \to W$ be a linear map. Then $\ker T$ is a subspace of $V$, and $\operatorname{im} T$ is a subspace of $W$.
\end{proposition}

\begin{proof}
  For $\ker T$:
  Let $u, v \in \ker T$ and $a, b \in F$. Then $T(au + bv) = aT(u) + bT(v) = a \cdot 0 + b \cdot 0 = 0$, so $au + bv \in \ker T$. Thus, $\ker T$ is a subspace of $V$.

  For $\operatorname{im} T$:
  Let $w_1, w_2 \in \operatorname{im} T$, so $w_1 = T(v_1)$, $w_2 = T(v_2)$ for some $v_1, v_2 \in V$. For $a, b \in F$, $a w_1 + b w_2 = a T(v_1) + b T(v_2) = T(a v_1 + b v_2) \in \operatorname{im} T$. Thus, $\operatorname{im} T$ is a subspace of $W$.
\end{proof}

\begin{proposition}
  A linear map $T: V \to W$ is injective if and only if $\ker T = \{0\}$.
\end{proposition}

\begin{theorem}
  Let $T: V \to W$ be a linear map between vector spaces, with $V$ finite-dimensional. Then
  \[
    \dim V = \dim (\ker T) + \dim (\operatorname{im} T).
  \]
\end{theorem}

\begin{definition}[Rank]
  Let $T: V \to W$ be a linear map between finite-dimensional vector spaces. The \textbf{rank} of $T$, denoted $\operatorname{rk}(T)$, is defined as
  \[
    \operatorname{rk}(T) := \dim(\operatorname{im} T).
  \]
  For a matrix $A \in M_{m \times n}(F)$, the rank $\operatorname{rk}(A)$ is defined as the rank of the associated linear map $T_A: F^n \to F^m$.
\end{definition}

\begin{proposition}
  Let $A' \in M_{m \times n}(F)$ be the row echelon form of $A \in M_{m \times n}(F)$ obtained by elementary row operations. Then the rank of $A$, $\operatorname{rk}(A)$, equals the number $r$ of pivots (leading ones) in $A'$.
\end{proposition}

\begin{proof}
  Elementary row operations do not change the row space of $A$, so $\operatorname{rk}(A) = \operatorname{rk}(A')$. In row echelon form, the number of pivots equals the dimension of the row (or column) space, which is the rank.
\end{proof}

\begin{proposition}
  Let $A \in M_{m \times m}(F)$. The following statements are equivalent:
  \begin{enumerate}
    \item[(i)] $A$ is invertible;
    \item[(ii)] For any column vector $v \in F^m$, $Av = 0$ if and only if $v = 0$;
    \item[(iii)] $\operatorname{rk}(A) = m$;
    \item[(iv)] $A$ can be written as a product of elementary matrices.
  \end{enumerate}
  Therefore, $A \in M_{m \times m}(F)$ is invertible if and only if it is left invertible, if and only if it is right invertible.
\end{proposition}

\begin{proof}
  (i) $\Rightarrow$ (ii): If $A$ is invertible and $Av = 0$, then $v = A^{-1}Av = A^{-1}0 = 0$.

  (ii) $\Rightarrow$ (iii): If $Av = 0$ only for $v = 0$, the columns of $A$ are linearly independent, so $\operatorname{rk}(A) = m$.

  (iii) $\Rightarrow$ (iv): If $\operatorname{rk}(A) = m$, $A$ can be reduced to the identity matrix by elementary row operations, so $A$ is a product of elementary matrices.

  (iv) $\Rightarrow$ (i): Elementary matrices are invertible, so their product $A$ is invertible.

  The equivalence of left and right invertibility for square matrices follows from these properties.
\end{proof}

\section{Change of Basis: Matrix Conjugation and Equivalence}

\begin{lemma}
  The map $P_{\mathbf{v}}^{\mathbf{v}'}: F^n \to F^n$ defined by change of basis from $\mathbf{v}$ to $\mathbf{v}'$ is a vector space automorphism of $F^n$. Its inverse is $P_{\mathbf{v}'}^{\mathbf{v}}$.
\end{lemma}

\begin{proof}
  By definition, $P_{\mathbf{v}}^{\mathbf{v}'}$ is invertible, with $P_{\mathbf{v}'}^{\mathbf{v}}$ as its inverse, since changing from basis $\mathbf{v}$ to $\mathbf{v}'$ and then back recovers the original coordinates. Thus, $P_{\mathbf{v}}^{\mathbf{v}'}$ is an automorphism.
\end{proof}

\begin{theorem}
  Let $V$ and $W$ be finite-dimensional vector spaces with ordered bases $\mathbf{v}, \mathbf{v}'$ for $V$ and $\mathbf{w}, \mathbf{w}'$ for $W$. For any $T \in \operatorname{Hom}(V, W)$, we have
  \[
    \mathcal{M}_{\mathbf{w}'}^{\mathbf{v}'}(T) = P_{\mathbf{w}}^{\mathbf{w}'} \, \mathcal{M}_{\mathbf{w}}^{\mathbf{v}}(T) \, P_{\mathbf{v}}^{\mathbf{v}'}
    = (P_{\mathbf{w}'}^{\mathbf{w}})^{-1} \mathcal{M}_{\mathbf{w}}^{\mathbf{v}}(T) P_{\mathbf{v}}^{\mathbf{v}'}.
  \]
\end{theorem}

\begin{definition}[Matrix Conjugation (Similarity)]
  Let $\mathbf{A}, \mathbf{B} \in M_{n \times n}(F)$. If there exists an invertible matrix $P \in M_{n \times n}(F)$ such that
  \[
    \mathbf{B} = P^{-1} \mathbf{A} P,
  \]
  then $\mathbf{A}$ and $\mathbf{B}$ are called \textbf{conjugate} or \textbf{similar} matrices.
\end{definition}

\begin{proposition}
  Conjugation is an equivalence relation on $M_{n \times n}(F)$.
\end{proposition}

\begin{definition}[Matrix Equivalence]
  Matrices $A, B \in M_{m \times n}(F)$ are called \textbf{equivalent} if there exist invertible matrices $Q \in M_{m \times m}(F)$ and $P \in M_{n \times n}(F)$ such that
  \[
    B = Q A P.
  \]
\end{definition}

\begin{proposition}
  Two matrices $A, B \in M_{m \times n}(F)$ are equivalent if and only if $\operatorname{rk}(A) = \operatorname{rk}(B)$.
\end{proposition}

\begin{theorem}
  For any matrix $A \in M_{m \times n}(F)$, the row rank of $A$ equals the column rank of $A$.
\end{theorem}

\begin{definition}
  A matrix $A \in M_{m \times n}(F)$ is called \textbf{full rank} if $\operatorname{rk}(A) = \min\{m, n\}$.
\end{definition}

\section{Direct Sum Decomposition}

\begin{definition}
  Let $V$ be a vector space over a field $F$, and let $(V_i)_{i \in I}$ be a family of subspaces of $V$. The sum of these subspaces is defined as
  \[
    \sum_{i \in I} V_i := \left\{ \sum_{i \in I} v_i \in V : v_i \in V_i, \text{ and only finitely many } v_i \neq 0 \right\}.
  \]
  For finitely many subspaces, we write $V_1 + \cdots + V_n$ for their sum. By convention, the sum over the empty index set $I = \emptyset$ is the zero subspace $\{0\}$.
\end{definition}

\begin{proposition}
  Let $(V_i)_{i \in I}$ be a family of subspaces of an $F$-vector space $V$, with $I \neq \emptyset$. If for every $i \in I$,
  \[
    V_i \cap \sum_{j \neq i} V_j = \{0\},
  \]
  then we denote $\sum_{i \in I} V_i$ by $\bigoplus_{i \in I} V_i$, called the (internal) \textbf{direct sum} of $(V_i)_{i \in I}$, and each $V_i$ is called a \textbf{direct summand}.

  This condition holds if and only if the canonical map from the external direct sum $\bigoplus_{i \in I} V_i$ to $\sum_{i \in I} V_i$ is an isomorphism.
\end{proposition}

\begin{proposition}
  Let $V$ be a vector space over a field $F$, and $V_0 \subset V$ any subspace. Then there exists a subspace $V_1 \subset V$ such that
  \[
    V = V_0 \oplus V_1.
  \]
\end{proposition}

\begin{proof}
  Let $\{v_1, \ldots, v_k\}$ be a basis for $V_0$. Extend this to a basis $\{v_1, \ldots, v_k, w_1, \ldots, w_m\}$ for $V$. Let $V_1 = \operatorname{span}\{w_1, \ldots, w_m\}$. Then every $v \in V$ can be uniquely written as $v = v_0 + v_1$ with $v_0 \in V_0$, $v_1 \in V_1$, so $V = V_0 \oplus V_1$.
\end{proof}

\section{Block Matrix Operations}

\begin{definition}
  Let $T: V \to W$ be a linear map, where $V = V_1 \oplus \cdots \oplus V_n$ and $W = W_1 \oplus \cdots \oplus W_m$ are direct sum decompositions. The matrix $A := M(T)$, with respect to these decompositions, can be partitioned into $m \times n$ blocks as follows:
  \[
    A =
    \begin{pmatrix}
      A_{11} & \cdots & A_{1n} \\
      \vdots & \ddots & \vdots \\
      A_{m1} & \cdots & A_{mn}
    \end{pmatrix}
  \]
  where $A_{ij} := M(T_{ij})$ is the matrix of the component map $T_{ij}: V_j \to W_i$.

  A matrix $A$ with such a partition is called a \textbf{block matrix}, and each $A_{ij}$ is called its $(i,j)$-block.
\end{definition}

Let $V$ be a vector space over $F$ and $U$ a subspace. For any $v \in V$, the equivalence class of $v$ modulo $U$ is denoted by
\[
  v + U := \{ v + u : u \in U \}.
\]
Such subsets are called \textbf{cosets} of $U$ in $V$, and $v$ is called a representative of the coset. We have $v + U = v' + U$ if and only if $v - v' \in U$.

\begin{definition}[Quotient Space]
  Let $U$ be a subspace of an $F$-vector space $V$. Define the \textbf{quotient space}
  \[
    V/U := \{ v + U : v \in V \}
  \]
  as the set of all cosets of $U$ in $V$.

  On $V/U$, define the following operations:
  \begin{itemize}
    \item Addition: $(v_1 + U) + (v_2 + U) := (v_1 + v_2) + U$, for $v_1, v_2 \in V$;
    \item Scalar multiplication: $t(v + U) := (tv) + U$, for $t \in F$, $v \in V$;
    \item Zero element: $0_{V/U} := U = 0_V + U$ (the coset of $0$).
  \end{itemize}

  With these operations, $(V/U, +, \cdot, 0_{V/U})$ is a vector space over $F$, called the \textbf{quotient space} of $V$ by $U$.
\end{definition}

\begin{definition}[Cokernel]
  Let $T: V \to W$ be a linear map. The \textbf{cokernel} of $T$ is defined as the quotient space of $W$ by the image of $T$:
  \[
    \operatorname{coker}(T) := W / \operatorname{im}(T).
  \]
\end{definition}

\begin{proposition}
  A linear map $T: V \to W$ is surjective if and only if $\operatorname{coker}(T) = \{0\}$.
\end{proposition}

\begin{proof}
  If $T$ is surjective, then $\operatorname{im}(T) = W$, so $W / \operatorname{im}(T) = W / W = \{0\}$.

  Conversely, if $\operatorname{coker}(T) = \{0\}$, then $W / \operatorname{im}(T) = \{0\}$, so $\operatorname{im}(T) = W$, i.e., $T$ is surjective.
\end{proof}

\begin{proposition}
  Let $U$ be a subspace of a vector space $V$, and let $T: V \to W$ be a linear map.
  \begin{enumerate}
    \item[(i)] If $U \subset \ker(T)$, then there exists a unique linear map $\overline{T}: V/U \to W$ such that the following diagram commutes:
      \[
        \begin{tikzcd}
          V \arrow[r, "T"] \arrow[d, "q"'] & W \\
          V/U \arrow[ur, "\overline{T}"', dashed]
        \end{tikzcd}
      \]
      where $q: V \to V/U$ is the quotient map. Explicitly, $\overline{T}(v + U) = T(v)$.

    \item[(ii)] If $U = \ker(T)$ and $W = \operatorname{im}(T)$, then $\overline{T}: V/U \to W$ is an isomorphism of vector spaces.
  \end{enumerate}
\end{proposition}

\begin{proof}
  (i) If $U \subset \ker(T)$, then $T(v) = T(v')$ whenever $v - v' \in U$, so $T$ is constant on cosets $v + U$. Thus, $\overline{T}(v + U) := T(v)$ is well-defined and linear. Uniqueness follows since $q$ is surjective.

  (ii) If $U = \ker(T)$ and $W = \operatorname{im}(T)$, then $\overline{T}$ is injective (since $\ker(\overline{T}) = \{U\}$) and surjective (since $T$ is surjective onto $W$), so it is an isomorphism.
\end{proof}

\begin{proposition}
  Let $U$ be a subspace of $V$. Let $\overline{V} := V/U$, and let $q: V \to \overline{V}$ be the quotient map. There is a bijection
  \[
    \left\{
      W \subset V : W \text{ is a subspace},\, W \supset U
    \right\}
    \longleftrightarrow
    \left\{
      \overline{W} \subset \overline{V} : \overline{W} \text{ is a subspace}
    \right\}
  \]
  given by $W \mapsto \overline{W} := q(W)$ and $\overline{W} \mapsto W := q^{-1}(\overline{W})$.

  This bijection has the following properties:
  \begin{itemize}
    \item It is strictly order-preserving: $W_1 \supset W_2$ if and only if $\overline{W}_1 \supset \overline{W}_2$.
    \item If $W$ corresponds to $\overline{W}$, then there is a natural isomorphism
      \[
        V/W \cong \overline{V}/\overline{W}, \quad v + W \mapsto q(v) + \overline{W}.
      \]
      If we write $\overline{W} = W/U$, this isomorphism can be viewed as
      \[
        V/W \cong (V/U)/(W/U).
      \]
  \end{itemize}
\end{proposition}

\begin{proposition}
  Let $V, W$ be subspaces of a vector space. Then there is a natural isomorphism
  \[
    V / (V \cap W) \cong (V + W) / W
  \]
  given by $v + (V \cap W) \mapsto v + W$ for $v \in V$.
\end{proposition}
