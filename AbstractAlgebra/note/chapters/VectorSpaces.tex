\chapter{Vector Spaces and linear mappings}
\label{chap:VectorSpaces}

Broadly speaking, a vector space over a field $F$ refers to a set $V$ together with two operations:
\begin{itemize}
  \item Vector addition $+: V \times V \rightarrow V$, denoted $(v_1, v_2) \mapsto v_1 + v_2$, satisfying associativity, commutativity, and the existence of inverses;
  \item Scalar multiplication $\cdot: F \times V \rightarrow V$, denoted $(t, v) \mapsto t \cdot v = tv$, satisfying associativity and distributivity over addition.
\end{itemize}

If a mapping $T: V \rightarrow W$ between vector spaces satisfies the identities
\begin{align*}
  T(v_1 + v_2) &= T(v_1) + T(v_2),\\
  T(tv) &= tT(v),
\end{align*}
then $T$ is called a linear mapping.

\section{Introduction: Back to the system of linear equations}

\begin{align}
  a_{11}X_1 + \cdots + a_{1n}X_n &= b_1 \nonumber\\
  a_{21}X_1 + \cdots + a_{2n}X_n &= b_2 \nonumber \\
  &\vdots\nonumber \\
  a_{m1}X_1 + \cdots + a_{mn}X_n &= b_m \label{eq:linear-system-2}
\end{align}

\begin{definition}
  Consider a system of $n$ linear equations over a field $F$ in the form (\ref{eq:linear-system-2}). If $b_1 = \cdots = b_m = 0$, then the system is called homogeneous.
\end{definition}

Given $n,m \in \mathbb{Z}_{\geq 1}$ and a family of coefficients $(a_{ij})_{1 \leq i \leq m, 1 \leq j \leq n}$ where $a_{ij} \in F$, define the mapping
\begin{align*}
  T: F^n &\rightarrow F^m\\
  (x_j)_{j=1}^n &\mapsto \left(\sum_{j=1}^n a_{1j}x_j, \ldots, \sum_{j=1}^n a_{mj}x_j\right).
\end{align*}

\begin{definition}
  Let $T: F^n \rightarrow F^m$ correspond to a homogeneous system of linear equations as described above. If $v_1, \ldots, v_h \in F^n$ are all solutions of the system, and every solution $x \in F^n$ can be uniquely expressed through addition and scalar multiplication as
  \begin{align}
    x = \sum_{i=1}^h t_i v_i, \quad t_1, \ldots, t_h \in F,
  \end{align}
  where the tuple $(t_1, \ldots, t_h)$ is uniquely determined by $x$, then $v_1, \ldots, v_h$ is called a fundamental system of solutions for the homogeneous system.
\end{definition}

\begin{proposition}
  Consider a homogeneous system of $n$ linear equations in the form (Eq.\ref{eq:linear-system-2}), where $\bf{b} = 0$. If the reduced row echelon matrix obtained by elimination has $r$ pivot elements, then the corresponding homogeneous system has a fundamental system of solutions $v_1, \ldots, v_{n-r}$.
\end{proposition}

\section{Vector spaces}

\begin{definition}
  A vector space over a field $F$, also called an $F$-vector space, is a tuple $(V, +, \cdot, 0_V)$ where $V$ is a set, $0_V \in V$, and operations $+: V \times V \rightarrow V$ and $\cdot: F \times V \rightarrow V$ are written as $(u,v) \mapsto u + v$ and $(t,v) \mapsto t \cdot v$ respectively, satisfying the following conditions:

  1. Addition satisfies:
  \begin{itemize}
    \item Associativity: $(u + v) + w = u + (v + w)$;
    \item Identity element: $v + 0_V = v = 0_V + v$;
    \item Commutativity: $u + v = v + u$;
    \item Additive inverse: For every $v$, there exists $-v$ such that $v + (-v) = 0_V$.
  \end{itemize}

  2. Scalar multiplication, often written as $tv$ instead of $t \cdot v$, satisfies:
  \begin{itemize}
    \item Associativity: $s \cdot (t \cdot v) = (st) \cdot v$;
    \item Identity property: $1 \cdot v = v$, where $1$ is the multiplicative identity in $F$.
  \end{itemize}

  3. Scalar multiplication distributes over addition:
  \begin{itemize}
    \item First distributive property: $(s + t) \cdot v = s \cdot v + t \cdot v$;
    \item Second distributive property: $s \cdot (u + v) = s \cdot u + s \cdot v$.
  \end{itemize}

  Where $u, v, w$ (or $s, t$) represent arbitrary elements of $V$ (or $F$). When there is no risk of confusion, we denote $0_V$ simply as $0$, write $u + (-v)$ as $u - v$, and refer to the structure $(V, +, \cdot, 0)$ simply as $V$.
\end{definition}

\begin{definition}
  Let $V$ be an $F$-vector space. If a subset $V_0$ of $V$ contains $0$ and is closed under addition and scalar multiplication, then $(V_0, +, \cdot, 0)$ is also an $F$-vector space, called a subspace of $V$.
\end{definition}

\section{Matrix \& calculate}

\begin{definition}
  Let $m,n \in \mathbb{Z}_{\geq 1}$. An $m \times n$ matrix over a field $F$ is a rectangular array
  \begin{align*}
    A = (a_{ij})_{1 \leq i \leq m, 1 \leq j \leq n} =
    \begin{pmatrix}
      a_{11} & \cdots & a_{1n} \\
      \vdots & \ddots & \vdots \\
      a_{m1} & \cdots & a_{mn}
    \end{pmatrix} =
    \begin{bmatrix}
      \cdots & \cdots & a_{ij} & \cdots & \cdots \\
    \end{bmatrix}
  \end{align*}

  \noindent where $a_{ij} \in F$ is called the $(i,j)$-entry or $(i,j)$-element of matrix $A$, with $i$ indicating the row and $j$ indicating the column. An $n \times n$ matrix is called a square matrix of order $n$.

  We denote the set of all $m \times n$ matrices over $F$ by $M_{m \times n}(F)$.
\end{definition}

\begin{proposition}
  The set $M_{m \times n}(F)$ equipped with standard addition and scalar multiplication forms an $F$-vector space. The zero element is the zero matrix, and the additive inverse of a matrix $A = (a_{ij})_{i,j}$ is $-A = (-a_{ij})_{i,j}$.
\end{proposition}

\begin{definition}[Matrix Multiplication]
  Matrix multiplication is a mapping defined as:
  \begin{align*}
    M_{m \times n}(F) \times M_{n \times r}(F) &\rightarrow M_{m \times r}(F)\\
    (A, B) &\mapsto AB
  \end{align*}

  If $A = (a_{ij})_{1 \leq i \leq m, 1 \leq j \leq n}$ and $B = (b_{jk})_{1 \leq j \leq n, 1 \leq k \leq r}$, then $AB = (c_{ik})_{1 \leq i \leq m, 1 \leq k \leq r}$, where:
  \begin{align*}
    c_{ik} := \sum_{j=1}^{n} a_{ij}b_{jk} =
    \begin{pmatrix} a_{i1} & \cdots & a_{in}
    \end{pmatrix}
    \begin{pmatrix} b_{1k} \\ \vdots \\ b_{nk}
    \end{pmatrix}
  \end{align*}

  This represents the dot product of the $i^{th}$ row of $A$ with the $k^{th}$ column of $B$.
\end{definition}

\begin{proposition}
  Matrix multiplication satisfies the following properties:
  \begin{itemize}
    \item Associativity: $(AB)C = A(BC)$;
    \item Distributivity: $A(B + C) = AB + AC$ and $(B + C)A = BA + CA$;
    \item Linearity: $A(tB) = t(AB) = (tA)B$;
  \end{itemize}
  where $t \in F$ and matrices $A, B, C$ are arbitrary, provided their dimensions make these operations valid.
\end{proposition}

\section{Bases \& Dimensions}

\begin{definition}
  Let $S$ be a subset of an $F$-vector space $V$.
  \begin{itemize}
    \item If $\langle S \rangle = V$, then $S$ is said to generate $V$, or $S$ is called a generating set of $V$.
    \item A linear relation in $S$ is an equation of the form
      \begin{align*}
        \sum_{s \in S} a_s s = 0
      \end{align*}
      This relation is called trivial if all coefficients $a_s$ are zero; otherwise, it is non-trivial. The set $S$ is linearly dependent if there exists a non-trivial linear relation among its elements; otherwise, $S$ is linearly independent.
    \item If $S$ is a linearly independent generating set, then $S$ is called a basis of $V$.
  \end{itemize}
\end{definition}

\begin{lemma}
  For any subset $S$ of an $F$-vector space $V$, the following statements are equivalent:
  \begin{enumerate}
    \item $S$ is a minimal generating set.
    \item $S$ is a basis.
    \item $S$ is a maximal linearly independent subset.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Let's prove the equivalence by showing $(1) \Rightarrow (2)$, $(2) \Rightarrow (3)$, and $(3) \Rightarrow (1)$.

  $(1) \Rightarrow (2)$: If $S$ is a minimal generating set, then $\langle S \rangle = V$. Suppose $S$ is not linearly independent. Then there exists some $s_0 \in S$ that can be expressed as a linear combination of other elements in $S$. But this means $S \setminus \{s_0\}$ still generates $V$, contradicting the minimality of $S$. Therefore, $S$ must be linearly independent, making it a basis.

  $(2) \Rightarrow (3)$: Let $S$ be a basis. Then $S$ is linearly independent and $\langle S \rangle = V$. To show that $S$ is maximal, suppose we add any vector $v \not\in S$ to form $S' = S \cup \{v\}$. Since $\langle S \rangle = V$, we have $v \in \langle S \rangle$, meaning $v$ can be written as a linear combination of elements in $S$. Therefore, $S'$ must be linearly dependent, proving that $S$ is a maximal linearly independent set.

  $(3) \Rightarrow (1)$: Let $S$ be a maximal linearly independent set. If $\langle S \rangle \neq V$, then there exists some $v \in V \setminus \langle S \rangle$. The set $S \cup \{v\}$ would still be linearly independent, contradicting the maximality of $S$. Therefore $\langle S \rangle = V$. Now suppose $S$ is not minimal. Then there exists a proper subset $S' \subset S$ with $\langle S' \rangle = V$. But this means some element in $S \setminus S'$ can be expressed as a linear combination of elements in $S'$, making $S$ linearly dependent, which is a contradiction. Thus, $S$ is a minimal generating set.
\end{proof}

\begin{proposition}
  Consider a family of $F$-vector spaces $V_i$, each with a given basis $B_i$, where $i$ ranges over a given index set $I$.
  Embed each $V_i$ as a subspace of the direct sum $\bigoplus_{j \in I} V_j$.
  Correspondingly, view each $B_i$ as a subset of $\bigoplus_{j \in I} V_j$.
  These subsets $B_i$ are pairwise disjoint, and $\bigcup_{i \in I} B_i$ forms a basis for $\bigoplus_{i \in I} V_i$.
\end{proposition}

\begin{definition}
  Every $F$-vector space $V$ has a basis. In fact, any linearly independent subset of $V$ can be extended to a basis.

  Furthermore, all bases of $V$ have the same cardinality. This common cardinality is called the dimension of $V$, denoted by $\dim_F V$ or simply $\dim V$.
\end{definition}

\begin{lemma}
  Let $\{s_1,...,s_n\}$ be a generating set for an $F$-vector space $V$. If $m > n$, then any collection of $m$ vectors $v_1,...,v_m \in V$ is linearly dependent.
\end{lemma}

\section{Linear mappings}

\begin{definition}[Linear Mapping]
  Let $V$ and $W$ be $F$-vector spaces. A mapping $T: V \rightarrow W$ is called a linear mapping (also known as a linear transformation or linear operator) if it satisfies:
  \begin{align*}
    T(v_1 + v_2) &= T(v_1) + T(v_2), & \forall v_1, v_2 \in V,\\
    T(tv) &= tT(v), & \forall t \in F, v \in V.
  \end{align*}
\end{definition}

\begin{lemma}
  If $T: U \rightarrow V$ and $S: V \rightarrow W$ are linear mappings, then their composition $S \circ T: U \rightarrow W$ is also a linear mapping.
\end{lemma}

\begin{proof}
  We need to verify that $S \circ T$ satisfies the two defining properties of linear mappings:

  1. For any $u_1, u_2 \in U$:
  \begin{align*}
    (S \circ T)(u_1 + u_2) &= S(T(u_1 + u_2)) \\
    &= S(T(u_1) + T(u_2)) \quad \text{(by linearity of $T$)} \\
    &= S(T(u_1)) + S(T(u_2)) \quad \text{(by linearity of $S$)} \\
    &= (S \circ T)(u_1) + (S \circ T)(u_2)
  \end{align*}

  2. For any $t \in F$ and $u \in U$:
  \begin{align*}
    (S \circ T)(tu) &= S(T(tu)) \\
    &= S(tT(u)) \quad \text{(by linearity of $T$)} \\
    &= tS(T(u)) \quad \text{(by linearity of $S$)} \\
    &= t(S \circ T)(u)
  \end{align*}

  Therefore, $S \circ T$ is a linear mapping.
\end{proof}

\begin{definition}
  If a linear mapping $T: V \rightarrow W$ is both left and right invertible, it is called an invertible linear mapping or an isomorphism.

  In this case, there exists a unique linear mapping $T^{-1}: W \rightarrow V$ such that $T^{-1} \circ T = \operatorname{id}_V$ and $T \circ T^{-1} = \operatorname{id}_W$. This mapping $T^{-1}$ is called the inverse of $T$; it is simultaneously the unique left inverse and the unique right inverse of $T$.
\end{definition}

\begin{definition}
  Let $V$ and $W$ be $F$-vector spaces. Define $\operatorname{Hom}(V,W)$ as the set of all linear mappings from $V$ to $W$. Addition and scalar multiplication are defined by:
  \begin{align*}
    (T_1 + T_2)(v) &= T_1(v) + T_2(v)\\
    (tT)(v) &= t \cdot T(v)
  \end{align*}

  The zero element of $\operatorname{Hom}(V,W)$ is the zero mapping $0: V \rightarrow W$.
\end{definition}

\section{Linear mappings to Matrix}

\begin{definition}
  Let $V$ be an $F$-vector space. We denote $\operatorname{End}(V) := \operatorname{Hom}(V,V)$, whose elements are called endomorphisms of $V$.
\end{definition}

\begin{corollary}
  Let $V$ be an $F$-vector space. Then $\operatorname{End}(V)$ forms a ring where addition is the addition of linear maps, and multiplication is the composition of linear maps $(S,T) \mapsto ST$. The zero element is the zero mapping, and the multiplicative identity is the identity mapping $\operatorname{id}_V$. Moreover, $\operatorname{End}(V)$ is the zero ring if and only if $V = \{0\}$.
\end{corollary}

\begin{theorem}
  Let $V$ and $W$ be finite-dimensional vector spaces with ordered bases $v_1,\ldots,v_n$ and $w_1,\ldots,w_m$ respectively, where $n,m \in \mathbb{Z}_{\geq 1}$. Then there exists a vector space isomorphism:
  \begin{align*}
    \mathcal{M}: \operatorname{Hom}(V,W) \stackrel{1:1}{\longrightarrow} M_{m\times n}(F)
  \end{align*}
  mapping $T \mapsto (a_{ij})_{1 \leq i \leq m, 1 \leq j \leq n}$, where $(a_{ij})_{i,j} = \mathcal{M}(T)$ is characterized by the property:
  \begin{align*}
    T(v_j) = \sum_{i=1}^m a_{ij}w_i, \quad 1 \leq j \leq n.
  \end{align*}
  Consequently, $\dim \operatorname{Hom}(V,W) = \dim V \cdot \dim W$.
\end{theorem}

\begin{theorem}
  Let $U$, $V$, and $W$ be finite-dimensional vector spaces with ordered bases $u_1,\ldots,u_r$, $v_1,\ldots,v_n$, and $w_1,\ldots,w_m$ respectively. The following diagram commutes:

  \begin{center}
    \begin{tikzcd}
      \operatorname{Hom}(V,W) \times \operatorname{Hom}(U,V) \arrow[r, "\text{Map composition}"] \arrow[d, "M \times M"'] & \operatorname{Hom}(U,W) \arrow[d, "M"] \\
      M_{m \times n}(F) \times M_{n \times r}(F) \arrow[r, "\text{Matrix multiplication}"'] & M_{m \times r}(F)
    \end{tikzcd}
  \end{center}

  In other words, $M(S \circ T) = M(S) \cdot M(T)$, where the left side represents composition of maps and the right side represents matrix multiplication.
\end{theorem}

\begin{definition}
  A matrix $A \in M_{m\times n}(F)$ is called left invertible (or right invertible) if there exists $B \in M_{n\times m}(F)$ such that $BA = 1_{n\times n}$ (or $AB = 1_{m\times m}$). Such a matrix $B$ is called a left inverse (or right inverse) of $A$.

  If $m = n$ and $A$ is both left and right invertible, then $A$ is called an invertible $n\times n$ matrix. In this case, there exists a unique matrix $A^{-1} \in M_{n\times n}(F)$ such that $A^{-1}A = 1_{n\times n} = AA^{-1}$. This matrix $A^{-1}$ serves simultaneously as both the unique left inverse and the unique right inverse of $A$.

  In other words, invertible matrices are precisely the invertible elements in the ring $M_{m\times m}(F)$.
\end{definition}

\section{Transpose of a Matrix and Dual Spaces}

\begin{definition}
  Let $R$ be a ring. For any $A = (a_{ij}) \in M_{m \times n}(R)$, the transpose of $A$, denoted ${}^t\!A$, is the $n \times m$ matrix $(a_{ji})$. That is,
  \[
    A =
    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}
    \implies
    {}^t\!A =
    \begin{pmatrix}
      a_{11} & a_{21} & \cdots & a_{m1} \\
      a_{12} & a_{22} & \cdots & a_{m2} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{1n} & a_{2n} & \cdots & a_{mn}
    \end{pmatrix}
  \]
\end{definition}

\begin{proposition}
  Let $A \in M_{m \times m}(F)$. Then $A$ is invertible if and only if ${}^t\!A$ is invertible. Moreover, in this case,
  \[
    ({}^t\!A)^{-1} = {}^t\!(A^{-1}).
  \]
\end{proposition}

\begin{proof}
  Suppose $A$ is invertible, so $A^{-1}A = AA^{-1} = I$. Taking transposes, $({}^t\!A^{-1})({}^t\!A) = {}^t\!(A A^{-1}) = {}^t\!I = I$, and $({}^t\!A)({}^t\!A^{-1}) = {}^t\!(A^{-1}A) = I$. Thus, ${}^t\!A$ is invertible and $({}^t\!A)^{-1} = {}^t\!(A^{-1})$.

  Conversely, if ${}^t\!A$ is invertible, the same argument applied to ${}^t\!A$ shows $A$ is invertible.
\end{proof}