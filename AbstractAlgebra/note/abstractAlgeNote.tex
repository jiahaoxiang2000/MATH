\documentclass[11pt,lang=en]{elegantbook}

\title{algebra note}
% \subtitle{Classic Elegant\LaTeX{} Template}

\author{isomo}
% \institute{Elegant\LaTeX{} Program}
\date{\today}
% \version{4.5}
% \bioinfo{Bio}{Information}

% \extrainfo{\textcolor{red}{\bfseries Caution: This template will no longer be maintained since January 1st, 2023.}}

% \logo{logo-blue.png}
\cover{cover.jpg}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

% \addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle

\chapter*{Preface}
\markboth{Introduction}{Introduction}

These abstract algebra notes primarily focus on self-study, with a writing style that deliberately maintains low information density and includes some redundancy for clarity.

My first encounter with abstract algebra was through an English textbook, which was heavily focused on theorem proofs. Progress was slow, and I struggled to see practical applications. After spending considerable time with this approach, I sought Chinese resources for potentially better learning methods. On Bilibili, I discovered Maki's abstract algebra lectures and accompanying notes, which provided an excellent introduction to the subject. However, the content still had some gaps. Later, after finding a recommended algebra book, ``Methods of Algebra" by Professor Li Wenwei, I began compiling these notes based on that foundation to aid my future studies. The ``Methods of Algebra" book is difficult, we math level maybe on the freshman level, so we find the ``Algebra Note" by the Professor Li Wenwei, which is more suitable for us.

\frontmatter
\tableofcontents

\mainmatter

\chapter{Introduction}

\section{What is Algebra?}

In light of this, classical algebra can be understood as the art of solving equations by:
\begin{itemize}
  \item Replacing specific numbers with variables
  \item Using operations such as transposition of terms
\end{itemize}

This traditional approach forms the foundation of algebraic manipulation and equation solving.

\begin{theorem}[Fundamental Theorem of Algebra]
  Let $f = X^n + a_{n-1}X^{n-1} + \cdots + a_0$ be a polynomial in $X$ with complex coefficients, where $n \in \mathbb{Z}_{\geq 1}$. Then there exist $x_1,\ldots,x_n \in \mathbb{C}$ such that:
  \[
    f = \prod_{k=1}^n (X-x_k)
  \]
  These $x_1,\ldots,x_n$ are precisely the complex roots of $f$ (counting multiplicity); they are unique up to reordering.
\end{theorem}

Now let us further explain the previously raised question: What is algebra?

\begin{itemize}
  \item \textbf{What is an equation?} \\
    An expression obtained through a finite number of basic operations: addition, subtraction, multiplication, and division (with non-zero denominators).

  \item \textbf{What are numbers?} \\
    At minimum, this includes common number systems like $\mathbb{Q}$, $\mathbb{R}$, and $\mathbb{C}$. All these systems support four basic operations, though division requires non-zero denominators. Note that $\mathbb{Z}$ is not included in this list, as division is not freely applicable in $\mathbb{Z}$.

  \item \textbf{What is the art of solving?} \\
    This involves:
    \begin{itemize}
      \item Determining whether equations have solutions
      \item Finding exact solutions when possible
      \item Developing efficient algorithms, Providing methods for approximating solutions
    \end{itemize}
\end{itemize}

\chapter{Sets mappings and relationships}

\section{Set Theory}

\begin{remark}
  Element of Set also one of Set.
\end{remark}

\begin{axiom}[Axiom of Extensionality]
  If two sets have the same elements, then they are equal.
  \[
    A=B \iff (A \subset B) \land (B \subset A)
  \]
\end{axiom}

\begin{axiom}[Axiom of Pairing]
  For any elements $x$ and $y$, there exists a set $\{x,y\}$ whose elements are exactly $x$ and $y$.
\end{axiom}

\begin{axiom}[Axiom Schema of Separation]
  Let $\mathcal{P} $ be a property of sets, and let $\mathcal{P}(u)$ denote that set $u$ satisfies property $\mathcal{P} $. Then for any set $X$, there exists a set $Y$ such that:
  \[
    Y = \{u \in X : \mathcal{P}(u)\}
  \]
\end{axiom}

\begin{axiom}[Axiom of Union]
  For any set $X$, there exists its union set $\bigcup X$ defined as:
  \[
    \bigcup X := \{u : \exists v \in X, u \in v\}
  \]
\end{axiom}

\begin{axiom}[Axiom of Power Set]
  For any set $X$, there exists its power set $P(X)$ defined as:
  \[
    P(X) := \{u : u \subset X\}
  \]
\end{axiom}

\begin{axiom}[Axiom of Infinity]
  There exists an infinite set. More precisely, there exists a set $X$ such that:
  \begin{enumerate}
    \item $\emptyset \in X$
    \item If $y \in X$, then $y \cup \{y\} \in X$
  \end{enumerate}
\end{axiom}

\begin{axiom}[Axiom Schema of Replacement]
  Let $\mathcal{F}$ be a function with domain set $X$. Then there exists a set $\mathcal{F}(X)$ defined as:
  \[
    \mathcal{F}(X) = \{\mathcal{F}(x) : x \in X\}
  \]
\end{axiom}

\begin{remark}
  The Replacement Axiom and the Separation Axiom Schema are to construct new sets from existing sets. Different is the Replacement can equal size of the set, but the Separation is a subset numbers of the set.
\end{remark}

\begin{definition}[Cartesian Product]
  For any two sets $A$ and $B$, their Cartesian product $A \times B$ (also called simply the product) consists of all ordered pairs $(a,b)$ where $a \in A$ and $b \in B$. In other words:
  \[
    A \times B := \{(a,b) : a \in A, b \in B\}
  \]
\end{definition}

\begin{axiom}[Axiom of Regularity]
  Every non-empty set contains an element which is minimal with respect to the membership relation $\in$.
\end{axiom}

\begin{axiom}[Axiom of Choice]
  Let $X$ be a set of non-empty sets. Then there exists a function $g : X \to \bigcup X$ (called a choice function) such that:
  \[
    \forall x \in X, g(x) \in x
  \]
\end{axiom}

\begin{example}[Symmetric Difference]
  The symmetric difference of sets $X$ and $Y$ is defined as $X \triangle Y := (X \setminus Y) \cup (Y \setminus X)$.
  Let's verify that $X \triangle Y = (X \cup Y) \setminus (X \cap Y)$.

  \begin{proof}
    Let $z$ be an arbitrary element. Then:
    \begin{align*}
      z \in X \triangle Y &\iff z \in (X \setminus Y) \cup (Y \setminus X) \\
      &\iff z \in X \setminus Y \text{ or } z \in Y \setminus X \\
      &\iff (z \in X \text{ and } z \notin Y) \text{ or } (z \in Y \text{ and } z \notin X) \\
      &\iff z \in X \cup Y \text{ and } z \notin X \cap Y \\
      &\iff z \in (X \cup Y) \setminus (X \cap Y)
    \end{align*}
    Therefore, $X \triangle Y = (X \cup Y) \setminus (X \cap Y)$.
  \end{proof}
\end{example}

\section{Mappings}

\begin{definition}[Mapping]
  Let $A$ and $B$ be sets. A mapping from $A$ to $B$ is written as $f : A \to B$ or $A \xrightarrow{f} B$.

  In set-theoretic language, we understand a mapping $f : A \to B$ as a subset of $A \times B$, denoted $\Gamma_f$, satisfying the following condition: for each $a \in A$, the set
  \[
    \{b \in B : (a,b) \in \Gamma_f\}
  \]
  is a singleton, whose unique element is denoted $f(a)$ and called the image of $a$ under $f$.
\end{definition}

\begin{definition}[Left and Right Inverses]
  Consider a pair of mappings $A \xrightarrow{f} B \xrightarrow{g} A$.
  If $g \circ f = \text{id}_A$, then:
  \begin{itemize}
    \item We call $g$ the left inverse of $f$
    \item We call $f$ the right inverse of $g$
  \end{itemize}
  A mapping with a left inverse (or right inverse) is called left invertible (or right invertible).
\end{definition}

\begin{example}[Composition of Invertible Maps]
  Let us show that the composition of two left (or right) invertible mappings is again left (or right) invertible.

  \begin{proof}
    Let $f: A \to B$ and $f': B \to C$ be left invertible mappings. Then:
    \begin{itemize}
      \item Let $g$ be left inverse of $f$, so $g \circ f = \text{id}_A$, Let $g'$ be left inverse of $f'$, so $g' \circ f' = \text{id}_B$
      \item Then for composition $f' \circ f$:
        \[
          (g \circ g') \circ (f' \circ f) = g \circ (g' \circ f') \circ f = g \circ f = \text{id}_A
        \]
      \item Therefore $g \circ g'$ is a left inverse of $f' \circ f$
    \end{itemize}
    The proof for right invertible mappings is similar.
  \end{proof}
\end{example}

\begin{proposition}{}{inject_left_inverse_equal}
  For a mapping $f : A \to B$ where $A$ is non-empty, the following are equivalent:
  \begin{enumerate}[(i)]
    \item $f$ is injective
    \item $f$ has a left inverse
    \item $f$ satisfies the left cancellation law
  \end{enumerate}

  Similarly, where $A$ is non-empty, the following are equivalent:
  \begin{enumerate}[(i)']
    \item $f$ is surjective
    \item $f$ has a right inverse
    \item $f$ satisfies the right cancellation law
  \end{enumerate}
\end{proposition}

\begin{proof}
  First, we prove the equivalence for injective properties:

  (i) $\implies$ (ii): Assume $f$ is injective.
  $\forall b \in \text{Im}(f)$, $\exists a \in A$, $f(a) = b$.
  Define $g: B \to A$ by $g(b) = a$ if $b \in \text{Im}(f)$, and arbitrary otherwise.
  Then $g \circ f = \text{id}_A$, so $g$ is left inverse.

  (ii) $\implies$ (iii): Assume $g \circ f = \text{id}_A$. If $fg_1 = fg_2$, then $g(fg_1)=g(fg_2) \iff (gf)g_1=(gf)g_2 \iff g_1=g_2$

  (iii) $\implies$ (i): Assume left cancellation, $fg_1=fg_2\implies g_1=g_2$,
  if $\forall a_1,a_2\in A,f(a_1)=f(a_2)$, then $f(a_1)=f(a_2)\implies a_1=a_2$.

  the proof for surjective properties is similar.
\end{proof}

\begin{definition}[Invertible Mapping]
  A mapping $f$ is called invertible if it is both left and right invertible. In this case, there exists a unique mapping $f^{-1}: B \to A$ such that:
  \[
    f^{-1} \circ f = \text{id}_A \quad \text{and} \quad f \circ f^{-1} = \text{id}_B
  \]
  This mapping $f^{-1}$ is called the inverse of $f$.
\end{definition}

\begin{proposition}
  Let $f : A \to B$ be an invertible mapping. Then:
  \begin{enumerate}
    \item $f^{-1} : B \to A$ is also invertible, and $(f^{-1})^{-1} = f$
    \item If $g : B \to C$ is also invertible, then the composition $g \circ f : A \to C$ is invertible, and
      \[
        (g \circ f)^{-1} = f^{-1} \circ g^{-1}
      \]
  \end{enumerate}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item Since $f \circ f^{-1} = \text{id}_B$ and $f^{-1} \circ f = \text{id}_A$,
      $f$ is both left and right inverse of $f^{-1}$, so $(f^{-1})^{-1} = f$

    \item For composition:
      \[
        (f^{-1} \circ g^{-1}) \circ (g \circ f) = f^{-1} \circ (g^{-1} \circ g) \circ f = f^{-1} \circ f = \text{id}_A
      \]
      Similarly, $(g \circ f) \circ (f^{-1} \circ g^{-1}) = \text{id}_C$
  \end{enumerate}
\end{proof}

\begin{proposition}
  A mapping $f$ is bijective if and only if it is invertible, in which case its inverse mapping is precisely the previously defined $f^{-1}$.
\end{proposition}

\begin{proof} There are easy to prove by the proposition~\ref{pro:inject_left_inverse_equal}

  ($\implies$) If $f$ is bijective: Being injective implies $f$ has a left inverse, Being surjective implies $f$ has a right inverse, Therefore $f$ is invertible.

  ($\impliedby$) If $f$ is invertible: Having left inverse implies $f$ is injective, Having right inverse implies $f$ is surjective, Therefore $f$ is bijective.
\end{proof}

\begin{definition}[Preimage]
  For a mapping $f : A \to B$ and $b \in B$, we denote:
  \[
    f^{-1}(b) := f^{-1}(\{b\}) = \{a \in A : f(a) = b\}
  \]
\end{definition}

\begin{remark}
  Note that this notation $f^{-1}(b)$ represents the preimage of $b$ under $f$, which exists even when $f$ is not invertible.
\end{remark}

\section{Product of Sets \& Disjoint Union}

\begin{definition}[Generalized Cartesian Product]
  Using the language of mappings, we define:
  \[
    \prod_{i \in I} A_i := \{f : I \to \bigcup_{i \in I} A_i \mid \forall i \in I, f(i) \in A_i\}
  \]

  Henceforth, we may write $f(i)$ as $a_i$, so elements of $\prod_{i \in I} A_i$ can be reasonably denoted as $(a_i)_{i \in I}$.

  For any $i \in I$, there is a mapping $p_i : \prod_{j \in I} A_j \to A_i$ defined by $p_i((a_j)_{j \in I}) = a_i$, called the $i$-th projection.
\end{definition}

\begin{remark}
  For easy to understand, The $\prod_{i \in I} A_i$ as the three domain space, the $(a_i)_{i\in I}$ as the one point in the three domain space, the $p_i$ as the projection from the three domain space to the one point.
\end{remark}

\begin{definition}[Disjoint Union and Partition]
  Let set $A$ be the union of a family of subsets $(A_i)_{i \in I}$, and suppose these subsets are pairwise disjoint, that is:
  \[
    \forall i,j \in I, i \neq j \implies A_i \cap A_j = \emptyset
  \]
  In this case, we say $A$ is the disjoint union of $(A_i)_{i \in I}$, or $(A_i)_{i \in I}$ is a partition of $A$, written as:
  \[
    A = \coprod_{i \in I} A_i
  \]
\end{definition}

\section{Structure of Order}

\begin{definition}[Binary Relation]
  A binary relation between sets $A$ and $B$ is any subset of $A \times B$. Let $R \subset A \times B$ be a binary relation. Then for all $a \in A$ and $b \in B$, we use the notation:
  \[
    aRb \text{ to represent } (a,b) \in R
  \]
  For convenience, when $A = B$, we call this a binary relation on $A$.
\end{definition}

\begin{definition}[Order Relations]
  Let $\preceq$ be a binary relation on set $A$. We call $\preceq$ a preorder and $(A,\preceq)$ a preordered set when:
  \begin{itemize}
    \item Reflexivity: For all $a \in A$, $a \preceq a$
    \item Transitivity: For all $a,b,c \in A$, if $a \preceq b$ and $b \preceq c$, then $a \preceq c$
  \end{itemize}

  If it also satisfies:
  \begin{itemize}
    \item Antisymmetry: For all $a,b \in A$, if $a \preceq b$ and $b \preceq a$, then $a = b$
  \end{itemize}
  then $\preceq$ is called a partial order and $(A,\preceq)$ is called a partially ordered set.

  A partially ordered set $(A,\preceq)$ is called a totally ordered set or chain if any two elements $a,b \in A$ are comparable, that is, either $a \preceq b$ or $b \preceq a$ holds.
\end{definition}

\begin{definition}[Order-Preserving Maps]
  Let $f : A \to B$ be a mapping between preordered sets. Then:
  \begin{itemize}
    \item $f$ is called order-preserving if:
      \[
        a \preceq a' \implies f(a) \preceq f(a') \text{ for all } a,a' \in A
      \]

    \item $f$ is called strictly order-preserving if:
      \[
        a \preceq a' \iff f(a) \preceq f(a') \text{ for all } a,a' \in A
      \]
  \end{itemize}
\end{definition}

\begin{definition}[Maximal, Minimal Elements and Bounds]
  Let $(A,\preceq)$ be a partially ordered set.
  \begin{itemize}
    \item An element $a_{\max} \in A$ is called a maximal element of $A$ if: there exists no $a \in A$ such that $a \succ a_{\max}$

    \item An element $a_{\min} \in A$ is called a minimal element of $A$ if: there exists no $a \in A$ such that $a \prec a_{\min}$
  \end{itemize}

  Furthermore, let $A'$ be a subset of $A$.
  \begin{itemize}
    \item An element $a \in A$ is called an upper bound of $A'$ in $A$ if: for all $a' \in A'$, $a' \preceq a$

    \item An element $a \in A$ is called a lower bound of $A'$ in $A$ if: for all $a' \in A'$, $a' \succeq a$
  \end{itemize}
\end{definition}

\begin{remark}
  we can use the tree structure to understand the maximal, minimal elements and bounds. the partial order like the link between the nodes, the maximal, minimal elements like the root nodes and leaf nodes.
\end{remark}

\begin{definition}[Well-Ordered Set]
  A totally ordered set $(A,\preceq)$ is called a well-ordered set if every non-empty subset $S \subseteq A$ has a minimal element.
\end{definition}

\section{Equivalence Relations and Quotient Sets}

\begin{definition}[Equivalence Relation]
  A binary relation $\sim$ on set $A$ is called an equivalence relation if it satisfies:
  \begin{itemize}
    \item Reflexivity: For all $a \in A$, $a \sim a$
    \item Symmetry: For all $a,b \in A$, if $a \sim b$ then $b \sim a$
    \item Transitivity: For all $a,b,c \in A$, if $a \sim b$ and $b \sim c$ then $a \sim c$
  \end{itemize}
\end{definition}

\begin{definition}[Equivalence Class]
  Let $\sim$ be an equivalence relation on set $A$. A non-empty subset $C \subset A$ is called an equivalence class if:
  \begin{itemize}
    \item Elements in $C$ are mutually equivalent: for all $x,y \in C$, $x \sim y$
    \item $C$ is closed under $\sim$: for all $x \in C$ and $y \in A$, if $x \sim y$ then $y \in C$
  \end{itemize}
  If $C$ is an equivalence class and $a \in C$, then $a$ is called a representative element of $C$.
\end{definition}

\begin{proposition}[Partition by Equivalence Classes]
  Let $\sim$ be an equivalence relation on set $A$. Then $A$ is the disjoint union of all its equivalence classes.
\end{proposition}

\begin{proof}
  Let $\{C_i\}_{i \in I}$ be the collection of all equivalence classes of $A$.
  \begin{enumerate}
    \item First, $A = \bigcup_{i \in I} C_i$ since every element belongs to its equivalence class
    \item For any distinct equivalence classes $C_i$ and $C_j$:
      If $x \in C_i \cap C_j$ and $x\neq \emptyset $, then $C_i = C_j$, this is a contradiction, so $C_i \cap C_j = \emptyset$.
    \item Therefore, $A = \coprod_{i \in I} C_i$
  \end{enumerate}
\end{proof}

\begin{definition}[Quotient Set]
  Let $\sim$ be an equivalence relation on a non-empty set $A$. The quotient set is defined as the following subset of the power set $\mathcal{P}(A)$:
  \[
    A/{\sim} := \{C \subset A : C \text{ is an equivalence class with respect to } \sim\}
  \]
  The quotient set comes with a quotient map $q: A \to A/{\sim}$ that maps each $a \in A$ to its unique equivalence class.
\end{definition}

\begin{remark}
  here the find the quotient set, we can use the boolean function symmetric for the equivalence relation, then we only travel the quotient set, which can reduce the travel space.
\end{remark}

\begin{proposition}[Universal Property of Quotient Maps]{universal_property_of_quotient_maps}
  Let $\sim$ be an equivalence relation on set $A$ and $q: A \to A/{\sim}$ be the corresponding quotient map. If a mapping $f: A \to B$ satisfies:
  \[
    a \sim a' \implies f(a) = f(a')
  \]
  then there exists a unique mapping $\bar{f}: (A/{\sim}) \to B$ such that:
  \[
    \bar{f} \circ q = f
  \]
\end{proposition}

\begin{proof}
  First, $\bar{f}$ is well-defined: for any $c \in A/{\sim}$. Then:
  \[
    \bar{f}(c) := f(a), a = q^{-1}(c)
  \]
  The proof of uniqueness: Assume $\bar{f}$ and $\bar{f}'$, then $\bar{f}\circ q=\bar{f'} \circ q$, the $q$ is surjective, so $\bar{f}=\bar{f}'$.
\end{proof}

\begin{proposition}
  For any mapping $f : A \to B$, define an equivalence relation $\sim_f$ on $A$ by:
  \[
    a \sim_f a' \iff f(a) = f(a')
  \]
  Then by Proposition~\ref{pro:universal_property_of_quotient_maps}, there exists a bijection:
  \[
    \bar{f} : (A/{\sim_f}) \xrightarrow{1:1} \text{im}(f)
  \]
\end{proposition}

\begin{proof}
  Let $q: A \to A/{\sim_f}$ be the quotient map. By the universal property:
  \begin{enumerate}
    \item Well-defined: If $[a] = [a']$, then $a \sim_f a'$, so $f(a) = f(a')$

    \item Injective: If $\bar{f}([a]) = \bar{f}([a'])$, then $f(a) = f(a')$,
      so $a \sim_f a'$, thus $[a] = [a']$

    \item Surjective: For any $b \in \text{im}(f)$, there exists $a \in A$
      with $f(a) = b$, so $\bar{f}([a]) = b$
  \end{enumerate}
  Therefore, $\bar{f}$ is a bijection between $A/{\sim_f}$ and $\text{im}(f)$.
\end{proof}

\section{positive integer to rational number }

\begin{definition}[Integers as Quotient Set]
  The set of integers $\mathbb{Z}$ is defined as the quotient set of $\mathbb{Z}_{\geq 0}^2$ under $\sim$. We temporarily denote the equivalence class containing $(m,n)$ in $\mathbb{Z}_{\geq 0}^2$ as $[m,n]$.
\end{definition}

\begin{remark}
  the $\sim$ relation is defined as $(m,n)\sim(m',n')\iff m+n'=m'+n \iff m-n=m'-n'$.
\end{remark}

\begin{definition}[Operations on Integer Equivalence Classes]
  For any elements $[m,n]$ and $[r,s]$ in $\mathbb{Z}$, define:
  \begin{align*}
    [m,n] + [r,s] &:= [m+r, n+s] \\
    [m,n] \cdot [r,s] &:= [mr+ns, nr+ms]
  \end{align*}
  By convention, multiplication $x \cdot y$ is often written simply as $xy$.
\end{definition}

\begin{definition}[Total Order on Integers]
  Define a total order $\leq$ on $\mathbb{Z}$ by:
  \[
    x \leq y \iff y-x \in \mathbb{Z}_{\geq 0}
  \]
\end{definition}

\begin{definition}[Rational Numbers]
  Define the set of rational numbers $\mathbb{Q}$ as the quotient set of $\mathbb{Z} \times (\mathbb{Z} \setminus \{0\})$ under the equivalence relation:
  \[
    (r,s) \sim (r',s') \iff rs' = r's
  \]
  We temporarily denote the equivalence class containing $(r,s)$ as $[r,s]$. Through the mapping $x \mapsto [x,1]$, we view $\mathbb{Z}$ as a subset of $\mathbb{Q}$.
\end{definition}

\begin{definition}[Total Order and Absolute Value on $\mathbb{Q}$]
  Define a total order on $\mathbb{Q}$ by:
  \begin{align*}
    [r,s] \geq 0 &\iff rs \geq 0 \\
    x \geq y &\iff x-y \geq 0
  \end{align*}

  For any $x \in \mathbb{Q}$, its absolute value $|x|$ is defined as:
  \[
    |x| =
    \begin{cases}
      x & \text{if } x \geq 0 \\
      -x & \text{if } x < 0
    \end{cases}
  \]
\end{definition}

\begin{proposition}
  Let $\mathbb{Q}^{\times} := \mathbb{Q} \setminus \{0\}$. For any $x \in \mathbb{Q}^{\times}$, there exists a unique $x^{-1} \in \mathbb{Q}^{\times}$ such that $xx^{-1} = 1$.
\end{proposition}

\begin{proof}
  For $x = [r,s] \in \mathbb{Q}^{\times}$, define $x^{-1} = [s,r]$ when $r > 0$ and $x^{-1} = [-s,-r]$ when $r < 0$.
  Then $xx^{-1} = 1$ and the uniqueness, here have the $x',x''$, then $x'x=1=x''x$, the $x$ have the right inverse, so $x'=x''$.
\end{proof}

\section{arithmetical}

\begin{definition}[Integer Multiples and Divisibility]
  For any $x \in \mathbb{Z}$, define:
  \[
    x\mathbb{Z} := \{xd : d \in \mathbb{Z}\}
  \]
  which consists of all multiples of $x$.

  For $x,y \in \mathbb{Z}$:
  \begin{itemize}
    \item We say $x$ divides $y$, written $x\mid y$, if $y \in x\mathbb{Z}$
    \item Otherwise, we write $x \nmid y$
    \item When $x\mid y$, we call $x$ a factor or divisor of $y$
  \end{itemize}
\end{definition}

\begin{proposition}[Division Algorithm]
  For any integers $a,d \in \mathbb{Z}$ where $d \neq 0$, there exist unique integers $q,r \in \mathbb{Z}$ such that:
  \begin{align*}
    a &= dq + r \\
    0 &\leq r < |d|
  \end{align*}
\end{proposition}

\begin{proof}
  \textbf{Existence:}  $\forall a,b,$ $\exists q\in \mathbb{Z}$, let exist $r = a - dq$ (here can use the modular equivalence relation), and $0\leq r<|d|$.

  \textbf{Uniqueness:}
  Suppose $a = dq_1 + r_1 = dq_2 + r_2$ with $0 \leq r_1,r_2 < |d|$
  \begin{itemize}
    \item Then $d(q_1 - q_2) = r_2 - r_1$
    \item $|r_2 - r_1| < |d|$
    \item Therefore $q_1 = q_2$ and $r_1 = r_2$
  \end{itemize}
\end{proof}

\begin{lemma}[Generator of Integer Ideals]
  Let $I$ be a non-empty subset of $\mathbb{Z}$ satisfying:
  \begin{enumerate}
    \item If $x,y \in I$, then $x + y \in I$
    \item If $a \in \mathbb{Z}$ and $x \in I$, then $ax \in I$
  \end{enumerate}
  Then there exists a unique $g \in \mathbb{Z}_{\geq 0}$ such that $I = g\mathbb{Z}$.
\end{lemma}

\begin{proof}
  If $I = \{0\}$, take $g = 0$. Otherwise, let $g$ be the smallest positive element in $I$.

  For any $x \in I$, by division algorithm:
  \[
    x = gq + r \text{ where } 0 \leq r < g
  \]

  Then $r = x - gq \in I$ by properties of $I$. By minimality of $g$, we must have $r = 0$.
  Therefore $x \in g\mathbb{Z}$, so $I \subseteq g\mathbb{Z}$.

  Since $g \in I$, we have $g\mathbb{Z} \subseteq I$. Thus $I = g\mathbb{Z}$.

  Uniqueness follows from the fact that $g$ must be the smallest positive element in $I$.
\end{proof}

\begin{definition}{Greatest Common Divisor}{greatest_common_divisor}
  For any integers $a,b \in \mathbb{Z}$, the greatest common divisor of $a$ and $b$, denoted $\text{gcd}(a,b)$, is the unique positive integer $d$ such that:
  \begin{itemize}
    \item $d \mid a$ and $d \mid b$
    \item For any $d' \in \mathbb{Z}$, if $d' \mid a$ and $d' \mid b$, then $d' \mid d$
  \end{itemize}
\end{definition}

\begin{proposition}[Bézout's Identity]{bezouts_identity}
  For integers $x_1,\ldots,x_n$:
  \[
    \mathbb{Z}x_1 + \cdots + \mathbb{Z}x_n = \gcd(x_1,\ldots,x_n)\mathbb{Z}
  \]

  Consequently, $x_1,\ldots,x_n$ are coprime if and only if there exist $a_1,\ldots,a_n \in \mathbb{Z}$ such that:
  \[
    a_1x_1 + \cdots + a_nx_n = 1
  \]
\end{proposition}

\begin{proof}
  We proceed by induction on $n$.

  For $n=2$: Let $d = \gcd(x_1,x_2)$. By Euclidean algorithm, there exist $a_1,a_2 \in \mathbb{Z}$ such that:
  \[
    d = a_1x_1 + a_2x_2 \in \mathbb{Z}x_1 + \mathbb{Z}x_2
  \]
  Therefore $d\mathbb{Z} \subseteq \mathbb{Z}x_1 + \mathbb{Z}x_2$.

  Conversely, since $d|x_1$ and $d|x_2$, we have $\mathbb{Z}x_1 + \mathbb{Z}x_2 \subseteq d\mathbb{Z}$.

  For $n > 2$: Let $g = \gcd(x_1,\ldots,x_{n-1})$. By induction:
  \[
    \mathbb{Z}x_1 + \cdots + \mathbb{Z}x_{n-1} = g\mathbb{Z}
  \]
  Then:
  \[
    \mathbb{Z}x_1 + \cdots + \mathbb{Z}x_n = g\mathbb{Z} + \mathbb{Z}x_n = \gcd(g,x_n)\mathbb{Z} = \gcd(x_1,\ldots,x_n)\mathbb{Z}
  \]

  The corollary follows directly since $\gcd(x_1,\ldots,x_n) = 1$ if and only if they are coprime.
\end{proof}

\begin{definition}[Prime Numbers]
  Let $p \in \mathbb{Z} \setminus \{0,\pm1\}$. We say $p$ is a prime element if its only divisors are $\pm1$ and $\pm p$.
  A positive prime element is called a prime number.
\end{definition}

\begin{proposition}[Euclid's Lemma]{euclids_lemma}
  Let $p$ be a prime element. If $a,b \in \mathbb{Z}$ such that $p\mid ab$, then either $p\mid a$ or $p\mid b$.
\end{proposition}

\begin{proof}
  If $p \nmid a$, then $\gcd(p,a) = 1$ since $p$ is prime.
  By proposition~\ref{pro:bezouts_identity}, there exist $x,y \in \mathbb{Z}$ such that:
  \[
    px + ay = 1
  \]
  Multiply both sides by $b$:
  \[
    pbx + aby = b
  \]
  Since $p\mid ab$, $pbx+aby \in p\mathbb{Z}$, so $p\mid b$.
\end{proof}

\begin{theorem}[Fundamental Theorem of Arithmetic]
  Every non-zero integer $n \in \mathbb{Z}$ has a prime factorization:
  \[
    n = \pm p_1^{a_1}\cdots p_r^{a_r}
  \]
  where $r \in \mathbb{Z}_{\geq 0}$ (with the convention that the right side equals $\pm 1$ when $r=0$), $p_1,\ldots,p_r$ are distinct prime numbers, $a_1,\ldots,a_r \in \mathbb{Z}_{\geq 1}$, and this factorization is unique up to ordering.
\end{theorem}

\begin{proof}
  \textbf{Existence:} By induction on $|n|$
  \begin{itemize}
    \item Base case: When $|n|=1$, take $r=0$
    \item For $|n|>1$: Let $p$ be the smallest prime divisor of $n$
    \item Then $n = pm$ where $|m| < |n|$
    \item By induction, $m$ has prime factorization
    \item Combine $p$ with $m$'s factorization
  \end{itemize}

  \textbf{Uniqueness:} Suppose we have two factorizations:
  \[
    p_1^{a_1}\cdots p_r^{a_r} = q_1^{b_1}\cdots q_s^{b_s}
  \]
  \begin{itemize}
    \item By proposition~\ref{pro:euclids_lemma}, $p_1$ divides some $q_i$
    \item Since both are prime, $p_1 = q_i$
    \item Cancel and continue by induction
    \item Therefore $r=s$ and factorizations are same up to ordering
  \end{itemize}
\end{proof}

\begin{remark}
  For a prime number $p$, we use the notation $p^a \parallel n$ to indicate that $p^a|n$ but $p^{a+1} \nmid n$ (i.e., $p^a$ is the exact power of $p$ dividing $n$).
\end{remark}

\begin{corollary}
  Consider integers $n = \pm \prod_{i=1}^r p_i^{a_i}$ and $m = \pm \prod_{i=1}^r p_i^{b_i}$, where $p_1,\ldots,p_r$ are distinct primes and $a_i,b_i \in \mathbb{Z}_{\geq 0}$. Then:
  \[
    \gcd(n,m) = \prod_{i=1}^r p_i^{\min\{a_i,b_i\}}, \quad \text{lcm}(n,m) = \prod_{i=1}^r p_i^{\max\{a_i,b_i\}}
  \]
  Similar results hold for GCD and LCM of any number of positive integers.
\end{corollary}

\begin{theorem}[Euclid]{euclid}
  There are infinitely many prime numbers.
\end{theorem}

\begin{proof}
  Let $p_1,\ldots,p_n$ be any finite collection of primes.
  Consider $N = p_1\cdots p_n + 1$.
  Any prime factor $p$ of $N$ must be different from all $p_i$ (since dividing $N$ by any $p_i$ leaves remainder 1).
  Therefore, no finite collection can contain all primes.
\end{proof}

\section{congruence}

\chapter{Group Theory}

\section{Monoid Group}

\begin{definition}{monoid}{monoid}
  We say that $(S, \ast)$ is a monoid if the binary operation satisfies the associative law and has an identity element. That is,
  \[
    \forall x, y, z \in S, \quad x \ast (y \ast z) = (x \ast y) \ast z
  \]
  and
  \[
    \exists e \in S, \forall x \in S, \quad e \ast x = x \ast e = x
  \]
\end{definition}

\begin{definition}{commutative monoid}{commutative monoid}
  We say that $(S, \ast)$ is a commutative monoid if it is a monoid and the operation satisfies the commutative law. That is,
  \[
    \forall x, y \in S, \quad x \ast y = y \ast x
  \]
\end{definition}

\begin{proposition}{unique of identity element}{uniqueness_of_identity_element}
  Let $(S, \cdot)$ be a monoid. Then the identity element is unique.
\end{proposition}
\begin{proof}
  Suppose that $e$ and $e'$ are both identity elements of $S$. Then
  \[
    e = e \cdot e' = e'
  \]
  so $e = e'$.
\end{proof}

\begin{proposition}{expand of associative law}{expand_of_associative_law}
  Let $x_1, \ldots, x_n, y_1, \ldots, y_m \in S$. Then
  \[
    x_1 \cdot x_2 \cdot \ldots \cdot x_n \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m = (x_1 \cdot x_2 \cdot \ldots \cdot x_n) \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
  \]
\end{proposition}
\begin{proof}
  We prove this by induction on $n$.

  \textbf{Base Case ($n = 1$):}
  When $n = 1$, the statement simplifies to:
  \[
    x_1 \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m = x_1 \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
  \]
  This is clearly true by the associative property of multiplication.

  \textbf{Inductive Step:}
  Assume the statement holds for $n = k$, that is:
  \[
    x_1 \cdot x_2 \cdot \ldots \cdot x_k \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m = (x_1 \cdot x_2 \cdot \ldots \cdot x_k) \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
  \]
  We need to show that the statement holds for $n = k + 1$. Consider:
  \[
    x_1 \cdot x_2 \cdot \ldots \cdot x_k \cdot x_{k+1} \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m
  \]
  By the associative property, we can regroup the terms as:
  \[
    (x_1 \cdot x_2 \cdot \ldots \cdot x_k ) \cdot (x_{k+1} \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m)
  \]
  Using the inductive hypothesis on the first $k$ terms, we have:
  \[
    (x_1 \cdot x_2 \cdot \ldots \cdot x_k) \cdot x_{k+1} \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m) = (x_1 \cdot x_2 \cdot \ldots \cdot x_k \cdot x_{k+1}) \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
  \]
  Thus, the statement holds for $n = k + 1$.

\end{proof}

\begin{proposition}
  Let $x \in S$ and $m,n \in \mathbb{N}$. Then
  \[
    x^{m+n} = x^m \cdot x^n
  \]
\end{proposition}
\begin{proof}
  We will prove this in three steps:

  \textbf{Step 1:} First, recall from Proposition~\ref{pro:expand_of_associative_law} that for any elements in $S$:
  \[
    x_1 \cdot x_2 \cdot \ldots \cdot x_n \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m = (x_1 \cdot x_2 \cdot \ldots \cdot x_n) \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
  \]

  \textbf{Step 2:} Now, consider the special case where all elements are equal to $x$:
  \begin{itemize}
    \item Let $x_1 = x_2 = \ldots = x_m = x$
    \item Let $y_1 = y_2 = \ldots = y_n = x$
  \end{itemize}

  \textbf{Step 3:} By definition of exponentiation in a monoid:
  \begin{align*}
    x^{m+n} &= \underbrace{x \cdot x \cdot \ldots \cdot x}_{m+n \text{ times}} \\
    &= (\underbrace{x \cdot x \cdot \ldots \cdot x}_{m \text{ times}}) \cdot (\underbrace{x \cdot x \cdot \ldots \cdot x}_{n \text{ times}}) \\
    &= x^m \cdot x^n
  \end{align*}

  Therefore, we have proved that $x^{m+n} = x^m \cdot x^n$ for all $x \in S$ and $m,n \in \mathbb{N}$.
\end{proof}

\begin{definition}{Submonoid}{submonoid}
  Let $(S,\cdot)$ be a monoid. If $T \subset S$, we say that $(T,\cdot)$ is a submonoid of $(S,\cdot)$ if:
  \begin{enumerate}
    \item The identity element $e \in T$
    \item $T$ is closed under multiplication, that is:
      \[
        \forall x,y \in T, \quad x \cdot y \in T
      \]
  \end{enumerate}
\end{definition}

\begin{proposition}
  If $(T,\cdot)$ is a submonoid of $(S,\cdot)$, then $(T,\cdot)$ is a monoid.
\end{proposition}
\begin{proof}
  We need to verify two properties:
  \begin{enumerate}
    \item The operation is associative in $T$: \\
      Since $T \subset S$ and $\cdot$ is associative in $S$, it is also associative in $T$.

    \item $T$ has an identity element: \\
      By definition of submonoid, the identity element $e \in T$.
  \end{enumerate}
  Therefore, $(T,\cdot)$ satisfies all properties of a monoid.
\end{proof}

\begin{definition}[Monoid Homomorphism]{monoid_homomorphism}
  Let $(S,\cdot)$ and $(T,\ast)$ be monoids, and let $f : S \to T$ be a mapping.
  We say $f$ is a monoid homomorphism if $f$ preserves multiplication and maps the identity element to the identity element. That is:
  \begin{enumerate}
    \item For all $x,y \in S$:
      \[
        f(x \cdot y) = f(x) \ast f(y)
      \]
    \item For the identity elements $e \in S$ and $e' \in T$:
      \[
        f(e) = e'
      \]
  \end{enumerate}
\end{definition}

\begin{remark}
  While a homomorphism preserves operations, an isomorphism represents complete structural equivalence. An isomorphism is first a \textbf{bijective mapping}, meaning it establishes a one-to-one correspondence between elements - essentially ``relabeling" elements uniquely. Beyond being bijective, an isomorphism preserves operations under this relabeling, implying that the only difference between two structures (like monoids) is their labeling.
\end{remark}

\begin{example}[~Different Types of Monoid Maps]
  Let's examine several maps between monoids:

  \begin{enumerate}
    \item \textbf{A homomorphism that is not an isomorphism:}
      Consider $f: (\mathbb{Z}, +) \to (\mathbb{Z}, +)$ defined by $f(n) = 2n$
      \begin{itemize}
        \item Preserves operation: $f(a + b) = 2(a + b) = 2a + 2b = f(a) + f(b)$
        \item Is injective: $f(a) = f(b) \implies 2a = 2b \implies a = b$
        \item Not surjective: odd numbers are not in the image
        \item Therefore: homomorphism but not isomorphism
      \end{itemize}
    \item \textbf{Non-isomorphic homomorphism:}
      Consider $h: (\mathbb{Z}, +) \to (\mathbb{Z}_2, +)$ defined by $h(n) = n \bmod 2$
      \begin{itemize}
        \item Preserves operation: $h(a + b) = (a + b) \bmod 2 = (a \bmod 2 + b \bmod 2) \bmod 2 = h(a) + h(b)$
        \item Not injective: $h(0) = h(2) = 0$
        \item Surjective: image is all of $\mathbb{Z}_2$
        \item Therefore: homomorphism but not isomorphism
      \end{itemize}
  \end{enumerate}
\end{example}

\begin{definition}[Generated Submonoid]
  Let $(S,\cdot)$ be a monoid and $A \subset S$ be a subset. The submonoid generated by $A$, denoted by $\langle A \rangle$, is defined as the intersection of all submonoids of $S$ containing $A$. That is:
  \[
    \langle A \rangle = \bigcap \{T \subset S : T \supset A, \text{ $T$ is a submonoid}\}
  \]
\end{definition}

\begin{proposition}
  Let $(S,\cdot)$ be a monoid and $A \subset S$ be a subset. Then $\langle A \rangle$ is also a submonoid. Therefore, it is the smallest submonoid containing $A$.
\end{proposition}
\begin{proof}
  We will prove this in two steps:

  \textbf{Step 1:} Show $\langle A \rangle$ contains the identity element

  Let $\{T_\alpha\}_{\alpha \in I}$ be the collection of all submonoids containing $A$,
  Each $T_\alpha$ contains the identity $e$ (by definition of submonoid),
  Therefore $e \in \bigcap_{\alpha \in I} T_\alpha = \langle A \rangle$

  \textbf{Step 2:} Show closure under multiplication

  Let $x, y \in \langle A \rangle = \bigcap_{\alpha \in I} T_\alpha$,
  Then $x, y \in T_\alpha$ for all $\alpha \in I$
  Since each $T_\alpha$ is a submonoid, $x \cdot y \in T_\alpha$ for all $\alpha \in I$,
  Therefore $x \cdot y \in \bigcap_{\alpha \in I} T_\alpha = \langle A \rangle$.

\end{proof}

\begin{definition}[Monoid Isomorphism]
  Let $(S,\cdot)$ and $(T,\ast)$ be monoids, and let $f : S \to T$ be a mapping. We say $f$ is a monoid isomorphism if $f$ is bijective and a homomorphism. That is:
  \begin{enumerate}
    \item $f$ is bijective (one-to-one and onto)
    \item For all $x,y \in S$:
      \[
        f(x \cdot y) = f(x) \ast f(y)
      \]
    \item For the identity elements $e \in S$ and $e' \in T$:
      \[
        f(e) = e'
      \]
  \end{enumerate}
\end{definition}

\begin{proposition}
  If $f : (S,\cdot) \to (T,\ast)$ is a monoid isomorphism, then $f^{-1} : T \to S$ is a monoid homomorphism. Therefore, $f^{-1}$ is also a monoid isomorphism.
\end{proposition}

\begin{proof}
  Since $f$ is an isomorphism, $f^{-1}$ exists and is bijective. We need to show:
  \begin{enumerate}
    \item $f^{-1}$ preserves operation:
      \begin{align*}
        f^{-1}(a \ast b) &= f^{-1}(f(f^{-1}(a)) \ast f(f^{-1}(b))) \\
        &= f^{-1}(f(f^{-1}(a) \cdot f^{-1}(b))) \\
        &= f^{-1}(a) \cdot f^{-1}(b)
      \end{align*}

    \item $f^{-1}$ preserves identity:
      \[
        f^{-1}(e') = e \text{ where $e'$ and $e$ are identity elements}
      \]
  \end{enumerate}
  Therefore, $f^{-1}$ is both a homomorphism and bijective, making it an isomorphism.
\end{proof}

\section{Group}

\begin{definition}[Invertible Element]{invertible_element}
  Let $(S,\cdot)$ be a monoid and $x \in S$. We say $x$ is invertible if and only if
  \[
    \exists y \in S, x \cdot y = y \cdot x = e
  \]
  where $y$ is called the inverse of $x$, denoted as $x^{-1}$.
\end{definition}

\begin{proposition}[Uniqueness of Inverse]{uniqueness_of_inverse}
  Let $(S,\cdot)$ be a monoid. If $x \in S$ is invertible, then its inverse is unique. That is, if $y,y' \in S$ are both inverses of $x$, then $y = y'$.
\end{proposition}

\begin{proof}
  Let $y$ and $y'$ be inverses of $x$. Then:
  \begin{align*}
    y &= y \cdot e \\
    &= y \cdot (x \cdot y') \\
    &= (y \cdot x) \cdot y' \\
    &= e \cdot y' \\
    &= y'
  \end{align*}
  Therefore, the inverse is unique.
\end{proof}

\begin{definition}[Group]{group}
  Let $(G,\cdot)$ be a monoid. We say it is a group if every element in $G$ is invertible.

  Equivalently, if $\cdot$ is a binary operation on $G$, we say $(G,\cdot)$ is a group, or $G$ forms a group under $\cdot$, when this operation satisfies:
  \begin{enumerate}
    \item Associativity: For all $x,y,z \in G$
      \[
        x \cdot (y \cdot z) = (x \cdot y) \cdot z
      \]

    \item Identity element: There exists $e \in G$ such that for all $x \in G$
      \[
        x \cdot e = e \cdot x = x
      \]

    \item Inverse elements: For each $x \in G$, there exists $y \in G$ such that
      \[
        x \cdot y = y \cdot x = e
      \]
  \end{enumerate}
\end{definition}

\begin{proposition}
  Let $(G,\cdot)$ be a group and $x \in G$. Then $(x^{-1})^{-1} = x$.
\end{proposition}

\begin{proof}
  Let $y = x^{-1}$. Then:
  \[
    y \cdot x = x \cdot y = e
  \]
  This shows that $x$ is the inverse of $y = x^{-1}$.
  Therefore, $(x^{-1})^{-1} = x$.
\end{proof}

\begin{proposition}[Inverse of Product]
  Let $(G,\cdot)$ be a group and $x,y \in G$. Then $(x \cdot y)^{-1} = y^{-1} \cdot x^{-1}$.
\end{proposition}

\begin{proof}
  We will show that $y^{-1} \cdot x^{-1}$ is the inverse of $x \cdot y$:
  \begin{align*}
    (x \cdot y)(y^{-1} \cdot x^{-1}) &= x \cdot (y \cdot y^{-1}) \cdot x^{-1} \\
    &= x \cdot e \cdot x^{-1} \\
    &= x \cdot x^{-1} \\
    &= e
  \end{align*}
  Similarly, $(y^{-1} \cdot x^{-1})(x \cdot y) = e$.
  Therefore, $(x \cdot y)^{-1} = y^{-1} \cdot x^{-1}$.
\end{proof}

\begin{definition}[Abelian Group]{abelian_group}
  Let $(G,\cdot)$ be a group. We say it is an abelian group, or commutative group, if the operation satisfies the commutative law:
  \[
    \forall x,y \in G, \quad x \cdot y = y \cdot x
  \]
\end{definition}

\begin{lemma}
  Let $(S,\cdot)$ be a monoid and let $G$ be the subset of all invertible elements in $S$. Then $(G,\cdot)$ is a group.
\end{lemma}

\begin{proof}
  We need to verify three group axioms:
  \begin{enumerate}
    \item Closure: If $x,y \in G$, then $x \cdot y \in G$ (as product of invertible elements is invertible)
    \item Identity: $e \in G$ (as $e$ is invertible)
    \item Inverse: If $x \in G$, then $x^{-1} \in G$ (by definition of invertible elements)
  \end{enumerate}
  Associativity is inherited from $S$. Therefore, $(G,\cdot)$ is a group.
\end{proof}

\begin{definition}[General Linear Group]{general_linear_group}
  The group of $n \times n$ invertible real matrices under matrix multiplication is called the general linear group of degree $n$ over the real numbers, denoted as $(GL(n,\mathbb{R}),\cdot)$. Since a matrix is invertible if and only if its determinant is nonzero:
  \[
    GL(n,\mathbb{R}) = \{ A \in M(n,\mathbb{R}) : \det(A) \neq 0 \}
  \]
\end{definition}

\begin{definition}[Special Linear Group]
  The special linear group of degree $n$ over the real numbers is the group of $n \times n$ real matrices with determinant exactly $1$ under matrix multiplication, denoted as $(SL(n,\mathbb{R}),\cdot)$. That is:
  \[
    SL(n,\mathbb{R}) = \{ A \in M(n,\mathbb{R}) : \det(A) = 1 \}
  \]
\end{definition}

\begin{definition}[Subgroup]{subgroup}
  Let $(G,\cdot)$ be a group and $H \subset G$. We say $H$ is a subgroup of $G$, denoted as $H < G$, if it contains the identity element and is closed under multiplication and inverse operations. That is:
  \begin{enumerate}
    \item $\forall x,y \in H, \quad x \cdot y \in H$ \quad (closure under multiplication)
    \item $\forall x \in H, \quad x^{-1} \in H$ \quad (closure under inverse)
    \item $e \in H$ \quad (contains identity)
  \end{enumerate}
\end{definition}

\begin{proposition}
  Let $(G,\cdot)$ be a group. If $H$ is a subgroup of $G$, then $(H,\cdot)$ is also a group.
\end{proposition}

\begin{proof}
  Since $H$ is a subgroup:
  \begin{enumerate}
    \item Associativity: Inherited from $G$
    \item Identity: $e \in H$ by definition of subgroup
    \item Inverse: For all $x \in H$, $x^{-1} \in H$ by definition of subgroup
    \item Closure: For all $x,y \in H$, $x \cdot y \in H$ by definition of subgroup
  \end{enumerate}
  Therefore, $(H,\cdot)$ satisfies all group axioms.
\end{proof}

\begin{proposition}
  For convenience, we can combine the first two conditions of a subgroup definition~\ref{def:subgroup} into one, reducing to two conditions:
  \begin{enumerate}
    \item $\forall x,y \in H, \quad x \cdot y^{-1} \in H$
    \item $e \in H$
  \end{enumerate}
  These conditions are equivalent to the original subgroup definition.
\end{proposition}

\begin{proof}

  ($\Rightarrow$)  $\forall y \in H$, $y^{-1} \in H$, then the closure under multiplication,  $\forall x,y,y^{-1} \in H$, $x \cdot y^{-1} \in H$

  ($\Leftarrow$) $\forall x,y \in H$, $x \cdot y^{-1} \in H$, let $x=e$, then have $\forall y\in H$, $y^{-1} \in H$; so $\forall x,y^{-1}\in H$, $x \cdot (y^{-1})^{-1} \in H$, then $x \cdot y \in H$.
\end{proof}

\begin{proposition}
  $(SL(n,\mathbb{R}),\cdot)$ is a group.
\end{proposition}

\begin{proof}
  We verify the group axioms:
  \begin{enumerate}
    \item Closure: If $A,B \in SL(n,\mathbb{R})$, then $\det(AB) = \det(A)\det(B) = 1 \cdot 1 = 1$, so $AB \in SL(n,\mathbb{R})$

    \item Identity: The identity matrix $I_n \in SL(n,\mathbb{R})$ since $\det(I_n) = 1$

    \item Inverse: If $A \in SL(n,\mathbb{R})$, then $\det(A^{-1}) = \frac{1}{\det(A)} = 1$, so $A^{-1} \in SL(n,\mathbb{R})$

    \item Associativity: Inherited from matrix multiplication
  \end{enumerate}
  Therefore, $(SL(n,\mathbb{R}),\cdot)$ is a group.
\end{proof}

\begin{definition}[Group Homomorphism]
  Let $(G,\cdot)$ and $(G',\ast)$ be groups, and let $f : G \to G'$ be a mapping. We say $f$ is a group homomorphism if it preserves the operation, that is:
  \[
    \forall x,y \in G, \quad f(x \cdot y) = f(x) \ast f(y)
  \]
\end{definition}

\begin{proposition}
  Let $f : (G,\cdot) \to (G',\ast)$ be a group homomorphism. Then:
  \begin{enumerate}
    \item $f(e) = e'$ (preserves identity)
    \item $f(x^{-1}) = f(x)^{-1}$ (preserves inverses)
  \end{enumerate}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item For identity element:
      \begin{align*}
        f(e) \ast f(e) &= f(e \cdot e) = f(e) \quad \text{left multiply by $f(e)^{-1}$} \\
        \therefore f(e) &= e'
      \end{align*}
    \item For inverse elements:
      \begin{align*}
        f(x) \ast f(x^{-1}) &= f(x \cdot x^{-1}) = f(e) = e' \quad \text{left multiply by $f(x)^{-1}$} \\
        \therefore f(x^{-1}) &= f(x)^{-1}
      \end{align*}
  \end{enumerate}
\end{proof}

%\problemset

% \printbibliography[heading=bibintoc, title=\ebibname]
% \appendix

% \chapter{Mathematical Tools}

% This appendix covers some of the basic mathematics used in econometrics. We briefly discuss the properties of summation operators, study the properties of linear and some nonlinear equations, and  also introduce some special functions that are common in econometrics applications, including quadratic functions and natural logarithms. The first four sections require only basic algebraic techniques. The fifth section briefly reviews differential Calculus Although Calculus is not necessary to understand much of this book, it is used in some of the end-of-chapter appendices and in some of the more advanced topics in part 3.

% \section{Summation Operator and Description Statistics}

% \textbf{Summation Operator} is an abbreviation used to express the summation of numbers, it plays an important role in statistics and econometrics analysis. If $\{x_i: i=1, 2, \ldots, n\}$ is a sequence of $n$ numbers, the summation of the $n$ numbers is:

\end{document}