\documentclass[11pt,lang=en]{elegantbook}

\title{algebra note}
% \subtitle{Classic Elegant\LaTeX{} Template}

\author{isomo}
% \institute{Elegant\LaTeX{} Program}
\date{\today}
% \version{4.5}
% \bioinfo{Bio}{Information}

% \extrainfo{\textcolor{red}{\bfseries Caution: This template will no longer be maintained since January 1st, 2023.}}

% \logo{logo-blue.png}
\cover{cover.jpg}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

% \addbibresource[location=local]{reference.bib} % bib

\begin{document}



\maketitle


\chapter*{Preface}
\markboth{Introduction}{Introduction}

These abstract algebra notes primarily focus on self-study, with a writing style that deliberately maintains low information density and includes some redundancy for clarity.

My first encounter with abstract algebra was through an English textbook, which was heavily focused on theorem proofs. Progress was slow, and I struggled to see practical applications. After spending considerable time with this approach, I sought Chinese resources for potentially better learning methods. On Bilibili, I discovered Maki's abstract algebra lectures and accompanying notes, which provided an excellent introduction to the subject. However, the content still had some gaps. Later, after finding a recommended algebra book, ``Methods of Algebra" by Professor Li Wenwei, I began compiling these notes based on that foundation to aid my future studies. The ``Methods of Algebra" book is difficult, we math level maybe on the freshman level, so we find the ``Algebra Note" by the Professor Li Wenwei, which is more suitable for us.

\frontmatter
\tableofcontents

\mainmatter

\chapter{Introduction}

\section{What is Algebra?}

In light of this, classical algebra can be understood as the art of solving equations by:
\begin{itemize}
  \item Replacing specific numbers with variables
  \item Using operations such as transposition of terms
\end{itemize}

This traditional approach forms the foundation of algebraic manipulation and equation solving.

\begin{theorem}[Fundamental Theorem of Algebra]
  Let $f = X^n + a_{n-1}X^{n-1} + \cdots + a_0$ be a polynomial in $X$ with complex coefficients, where $n \in \mathbb{Z}_{\geq 1}$. Then there exist $x_1,\ldots,x_n \in \mathbb{C}$ such that:
  \[
    f = \prod_{k=1}^n (X-x_k)
  \]
  These $x_1,\ldots,x_n$ are precisely the complex roots of $f$ (counting multiplicity); they are unique up to reordering.
\end{theorem}


Now let us further explain the previously raised question: What is algebra?

\begin{itemize}
  \item \textbf{What is an equation?} \\
    An expression obtained through a finite number of basic operations: addition, subtraction, multiplication, and division (with non-zero denominators).
    
  \item \textbf{What are numbers?} \\
    At minimum, this includes common number systems like $\mathbb{Q}$, $\mathbb{R}$, and $\mathbb{C}$. All these systems support four basic operations, though division requires non-zero denominators. Note that $\mathbb{Z}$ is not included in this list, as division is not freely applicable in $\mathbb{Z}$.
    
  \item \textbf{What is the art of solving?} \\
    This involves:
    \begin{itemize}
      \item Determining whether equations have solutions
      \item Finding exact solutions when possible
      \item Developing efficient algorithms, Providing methods for approximating solutions
    \end{itemize}
\end{itemize}


\chapter{Sets mappings and relationships}

\section{Set Theory}

\begin{remark}
  Element of Set also one of Set.
\end{remark}

\begin{axiom}[Axiom of Extensionality]
  If two sets have the same elements, then they are equal.
  \[
  A=B \iff (A \subset B) \land (B \subset A) 
  \]
\end{axiom}

\begin{axiom}[Axiom of Pairing]
  For any elements $x$ and $y$, there exists a set $\{x,y\}$ whose elements are exactly $x$ and $y$.
\end{axiom}



\begin{axiom}[Axiom Schema of Separation]
  Let $\mathcal{P} $ be a property of sets, and let $\mathcal{P}(u)$ denote that set $u$ satisfies property $\mathcal{P} $. Then for any set $X$, there exists a set $Y$ such that:
  \[
    Y = \{u \in X : \mathcal{P}(u)\}
  \]
\end{axiom}


\begin{axiom}[Axiom of Union]
  For any set $X$, there exists its union set $\bigcup X$ defined as:
  \[
    \bigcup X := \{u : \exists v \in X, u \in v\}
  \]
\end{axiom}

\begin{axiom}[Axiom of Power Set]
  For any set $X$, there exists its power set $P(X)$ defined as:
  \[
    P(X) := \{u : u \subset X\}
  \]
\end{axiom}

\begin{axiom}[Axiom of Infinity]
  There exists an infinite set. More precisely, there exists a set $X$ such that:
  \begin{enumerate}
    \item $\emptyset \in X$
    \item If $y \in X$, then $y \cup \{y\} \in X$
  \end{enumerate}
\end{axiom}

\begin{axiom}[Axiom Schema of Replacement]
  Let $\mathcal{F}$ be a function with domain set $X$. Then there exists a set $\mathcal{F}(X)$ defined as:
  \[
    \mathcal{F}(X) = \{\mathcal{F}(x) : x \in X\}
  \]
\end{axiom}

\begin{remark}
  The Replacement Axiom and the Separation Axiom Schema are to construct new sets from existing sets. Different is the Replacement can equal size of the set, but the Separation is a subset numbers of the set.
\end{remark}

\begin{definition}[Cartesian Product]
  For any two sets $A$ and $B$, their Cartesian product $A \times B$ (also called simply the product) consists of all ordered pairs $(a,b)$ where $a \in A$ and $b \in B$. In other words:
  \[
    A \times B := \{(a,b) : a \in A, b \in B\}
  \]
\end{definition}

\begin{axiom}[Axiom of Regularity]
  Every non-empty set contains an element which is minimal with respect to the membership relation $\in$. 
\end{axiom}

\begin{axiom}[Axiom of Choice]
  Let $X$ be a set of non-empty sets. Then there exists a function $g : X \to \bigcup X$ (called a choice function) such that:
  \[
    \forall x \in X, g(x) \in x
  \]
\end{axiom}

\begin{example}[Symmetric Difference]
  The symmetric difference of sets $X$ and $Y$ is defined as $X \triangle Y := (X \setminus Y) \cup (Y \setminus X)$. 
  Let's verify that $X \triangle Y = (X \cup Y) \setminus (X \cap Y)$.
  
  \begin{proof}
    Let $z$ be an arbitrary element. Then:
    \begin{align*}
      z \in X \triangle Y &\iff z \in (X \setminus Y) \cup (Y \setminus X) \\
      &\iff z \in X \setminus Y \text{ or } z \in Y \setminus X \\
      &\iff (z \in X \text{ and } z \notin Y) \text{ or } (z \in Y \text{ and } z \notin X) \\
      &\iff z \in X \cup Y \text{ and } z \notin X \cap Y \\
      &\iff z \in (X \cup Y) \setminus (X \cap Y)
    \end{align*}
    Therefore, $X \triangle Y = (X \cup Y) \setminus (X \cap Y)$.
  \end{proof}
\end{example}


\section{Mappings}

\begin{definition}[Mapping]
  Let $A$ and $B$ be sets. A mapping from $A$ to $B$ is written as $f : A \to B$ or $A \xrightarrow{f} B$.
  
  In set-theoretic language, we understand a mapping $f : A \to B$ as a subset of $A \times B$, denoted $\Gamma_f$, satisfying the following condition: for each $a \in A$, the set
  \[
    \{b \in B : (a,b) \in \Gamma_f\}
  \]
  is a singleton, whose unique element is denoted $f(a)$ and called the image of $a$ under $f$.
\end{definition}

\begin{definition}[Left and Right Inverses]
  Consider a pair of mappings $A \xrightarrow{f} B \xrightarrow{g} A$. 
  If $g \circ f = \text{id}_A$, then:
  \begin{itemize}
    \item We call $g$ the left inverse of $f$
    \item We call $f$ the right inverse of $g$
  \end{itemize}
  A mapping with a left inverse (or right inverse) is called left invertible (or right invertible).
\end{definition}

\begin{example}[Composition of Invertible Maps]
  Let us show that the composition of two left (or right) invertible mappings is again left (or right) invertible.
  
  \begin{proof}
    Let $f: A \to B$ and $f': B \to C$ be left invertible mappings. Then:
    \begin{itemize}
      \item Let $g$ be left inverse of $f$, so $g \circ f = \text{id}_A$, Let $g'$ be left inverse of $f'$, so $g' \circ f' = \text{id}_B$
      \item Then for composition $f' \circ f$:
        \[
          (g \circ g') \circ (f' \circ f) = g \circ (g' \circ f') \circ f = g \circ f = \text{id}_A
        \]
      \item Therefore $g \circ g'$ is a left inverse of $f' \circ f$
    \end{itemize}
    The proof for right invertible mappings is similar.
  \end{proof}
\end{example}


\chapter{Group Theory}


\section{Monoid Group}

\begin{definition}{monoid}{monoid}
  We say that $(S, \ast)$ is a monoid if the binary operation satisfies the associative law and has an identity element. That is,
  \[
  \forall x, y, z \in S, \quad x \ast (y \ast z) = (x \ast y) \ast z
  \]
  and
  \[
  \exists e \in S, \forall x \in S, \quad e \ast x = x \ast e = x
  \]
\end{definition}


\begin{definition}{commutative monoid}{commutative monoid}
  We say that $(S, \ast)$ is a commutative monoid if it is a monoid and the operation satisfies the commutative law. That is,
  \[
  \forall x, y \in S, \quad x \ast y = y \ast x
  \]
\end{definition}

\begin{proposition}{unique of identity element}{uniqueness_of_identity_element}
  Let $(S, \cdot)$ be a monoid. Then the identity element is unique.
\end{proposition}
\begin{proof}
  Suppose that $e$ and $e'$ are both identity elements of $S$. Then
  \[
  e = e \cdot e' = e'
  \]
  so $e = e'$.
\end{proof}

\begin{proposition}{expand of associative law}{expand_of_associative_law}
  Let $x_1, \ldots, x_n, y_1, \ldots, y_m \in S$. Then
  \[
  x_1 \cdot x_2 \cdot \ldots \cdot x_n \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m = (x_1 \cdot x_2 \cdot \ldots \cdot x_n) \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
  \]
\end{proposition}
\begin{proof}
We prove this by induction on $n$.

\textbf{Base Case ($n = 1$):}  
When $n = 1$, the statement simplifies to:
\[
x_1 \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m = x_1 \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
\]
This is clearly true by the associative property of multiplication.

\textbf{Inductive Step:}  
Assume the statement holds for $n = k$, that is:
\[
x_1 \cdot x_2 \cdot \ldots \cdot x_k \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m = (x_1 \cdot x_2 \cdot \ldots \cdot x_k) \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
\]
We need to show that the statement holds for $n = k + 1$. Consider:
\[
x_1 \cdot x_2 \cdot \ldots \cdot x_k \cdot x_{k+1} \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m
\]
By the associative property, we can regroup the terms as:
\[
(x_1 \cdot x_2 \cdot \ldots \cdot x_k ) \cdot (x_{k+1} \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m)
\]
Using the inductive hypothesis on the first $k$ terms, we have:
\[
(x_1 \cdot x_2 \cdot \ldots \cdot x_k) \cdot x_{k+1} \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m) = (x_1 \cdot x_2 \cdot \ldots \cdot x_k \cdot x_{k+1}) \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
\]
Thus, the statement holds for $n = k + 1$.

\end{proof}

\begin{proposition}
  Let $x \in S$ and $m,n \in \mathbb{N}$. Then
  \[
  x^{m+n} = x^m \cdot x^n
  \]
\end{proposition}
\begin{proof}
  We will prove this in three steps:

  \textbf{Step 1:} First, recall from Proposition~\ref{pro:expand_of_associative_law} that for any elements in $S$:
  \[
    x_1 \cdot x_2 \cdot \ldots \cdot x_n \cdot y_1 \cdot y_2 \cdot \ldots \cdot y_m = (x_1 \cdot x_2 \cdot \ldots \cdot x_n) \cdot (y_1 \cdot y_2 \cdot \ldots \cdot y_m)
  \]

  \textbf{Step 2:} Now, consider the special case where all elements are equal to $x$:
  \begin{itemize}
    \item Let $x_1 = x_2 = \ldots = x_m = x$
    \item Let $y_1 = y_2 = \ldots = y_n = x$
  \end{itemize}

  \textbf{Step 3:} By definition of exponentiation in a monoid:
  \begin{align*}
    x^{m+n} &= \underbrace{x \cdot x \cdot \ldots \cdot x}_{m+n \text{ times}} \\
    &= (\underbrace{x \cdot x \cdot \ldots \cdot x}_{m \text{ times}}) \cdot (\underbrace{x \cdot x \cdot \ldots \cdot x}_{n \text{ times}}) \\
    &= x^m \cdot x^n
  \end{align*}

  Therefore, we have proved that $x^{m+n} = x^m \cdot x^n$ for all $x \in S$ and $m,n \in \mathbb{N}$.
\end{proof}

\begin{definition}{Submonoid}{submonoid}
  Let $(S,\cdot)$ be a monoid. If $T \subset S$, we say that $(T,\cdot)$ is a submonoid of $(S,\cdot)$ if:
  \begin{enumerate}
    \item The identity element $e \in T$
    \item $T$ is closed under multiplication, that is:
    \[
      \forall x,y \in T, \quad x \cdot y \in T
    \]
  \end{enumerate}
\end{definition}

\begin{proposition}
  If $(T,\cdot)$ is a submonoid of $(S,\cdot)$, then $(T,\cdot)$ is a monoid.
\end{proposition}
\begin{proof}
  We need to verify two properties:
  \begin{enumerate}
    \item The operation is associative in $T$: \\
    Since $T \subset S$ and $\cdot$ is associative in $S$, it is also associative in $T$.
    
    \item $T$ has an identity element: \\
    By definition of submonoid, the identity element $e \in T$.
  \end{enumerate}
  Therefore, $(T,\cdot)$ satisfies all properties of a monoid.
\end{proof}

\begin{definition}[Monoid Homomorphism]{monoid_homomorphism}
  Let $(S,\cdot)$ and $(T,\ast)$ be monoids, and let $f : S \to T$ be a mapping. 
  We say $f$ is a monoid homomorphism if $f$ preserves multiplication and maps the identity element to the identity element. That is:
  \begin{enumerate}
    \item For all $x,y \in S$:
    \[
      f(x \cdot y) = f(x) \ast f(y) 
    \]
    \item For the identity elements $e \in S$ and $e' \in T$:
    \[
      f(e) = e' 
    \]
  \end{enumerate}
\end{definition}

\begin{remark}
  While a homomorphism preserves operations, an isomorphism represents complete structural equivalence. An isomorphism is first a \textbf{bijective mapping}, meaning it establishes a one-to-one correspondence between elements - essentially ``relabeling" elements uniquely. Beyond being bijective, an isomorphism preserves operations under this relabeling, implying that the only difference between two structures (like monoids) is their labeling.
\end{remark}

\begin{example}[~Different Types of Monoid Maps]
  Let's examine several maps between monoids:

  \begin{enumerate}
    \item \textbf{A homomorphism that is not an isomorphism:} 
     Consider $f: (\mathbb{Z}, +) \to (\mathbb{Z}, +)$ defined by $f(n) = 2n$
    \begin{itemize}
      \item Preserves operation: $f(a + b) = 2(a + b) = 2a + 2b = f(a) + f(b)$
      \item Is injective: $f(a) = f(b) \implies 2a = 2b \implies a = b$
      \item Not surjective: odd numbers are not in the image
      \item Therefore: homomorphism but not isomorphism
    \end{itemize}
    \item \textbf{Non-isomorphic homomorphism:} 
    Consider $h: (\mathbb{Z}, +) \to (\mathbb{Z}_2, +)$ defined by $h(n) = n \bmod 2$
    \begin{itemize}
      \item Preserves operation: $h(a + b) = (a + b) \bmod 2 = (a \bmod 2 + b \bmod 2) \bmod 2 = h(a) + h(b)$
      \item Not injective: $h(0) = h(2) = 0$
      \item Surjective: image is all of $\mathbb{Z}_2$
      \item Therefore: homomorphism but not isomorphism
    \end{itemize}
  \end{enumerate}
\end{example}

\begin{definition}[Generated Submonoid]
  Let $(S,\cdot)$ be a monoid and $A \subset S$ be a subset. The submonoid generated by $A$, denoted by $\langle A \rangle$, is defined as the intersection of all submonoids of $S$ containing $A$. That is:
  \[
    \langle A \rangle = \bigcap \{T \subset S : T \supset A, \text{ $T$ is a submonoid}\}
  \]
\end{definition}

\begin{proposition}
  Let $(S,\cdot)$ be a monoid and $A \subset S$ be a subset. Then $\langle A \rangle$ is also a submonoid. Therefore, it is the smallest submonoid containing $A$.
\end{proposition}
\begin{proof}
  We will prove this in two steps:

  \textbf{Step 1:} Show $\langle A \rangle$ contains the identity element 
  
  Let $\{T_\alpha\}_{\alpha \in I}$ be the collection of all submonoids containing $A$,
  Each $T_\alpha$ contains the identity $e$ (by definition of submonoid),
  Therefore $e \in \bigcap_{\alpha \in I} T_\alpha = \langle A \rangle$

  \textbf{Step 2:} Show closure under multiplication
  
     Let $x, y \in \langle A \rangle = \bigcap_{\alpha \in I} T_\alpha$,
     Then $x, y \in T_\alpha$ for all $\alpha \in I$
     Since each $T_\alpha$ is a submonoid, $x \cdot y \in T_\alpha$ for all $\alpha \in I$,
     Therefore $x \cdot y \in \bigcap_{\alpha \in I} T_\alpha = \langle A \rangle$.
  
\end{proof}

\begin{definition}[Monoid Isomorphism]
  Let $(S,\cdot)$ and $(T,\ast)$ be monoids, and let $f : S \to T$ be a mapping. We say $f$ is a monoid isomorphism if $f$ is bijective and a homomorphism. That is:
  \begin{enumerate}
    \item $f$ is bijective (one-to-one and onto) 
    \item For all $x,y \in S$:
    \[
      f(x \cdot y) = f(x) \ast f(y) 
    \]
    \item For the identity elements $e \in S$ and $e' \in T$:
    \[
      f(e) = e' 
    \]
  \end{enumerate}
\end{definition}

\begin{proposition}
  If $f : (S,\cdot) \to (T,\ast)$ is a monoid isomorphism, then $f^{-1} : T \to S$ is a monoid homomorphism. Therefore, $f^{-1}$ is also a monoid isomorphism.
\end{proposition}

\begin{proof}
  Since $f$ is an isomorphism, $f^{-1}$ exists and is bijective. We need to show:
  \begin{enumerate}
    \item $f^{-1}$ preserves operation:
    \begin{align*}
      f^{-1}(a \ast b) &= f^{-1}(f(f^{-1}(a)) \ast f(f^{-1}(b))) \\
      &= f^{-1}(f(f^{-1}(a) \cdot f^{-1}(b))) \\
      &= f^{-1}(a) \cdot f^{-1}(b)
    \end{align*}
    
    \item $f^{-1}$ preserves identity:
    \[
      f^{-1}(e') = e \text{ where $e'$ and $e$ are identity elements}
    \]
  \end{enumerate}
  Therefore, $f^{-1}$ is both a homomorphism and bijective, making it an isomorphism.
\end{proof}

\section{Group}

\begin{definition}[Invertible Element]{invertible_element}
  Let $(S,\cdot)$ be a monoid and $x \in S$. We say $x$ is invertible if and only if
  \[
    \exists y \in S, x \cdot y = y \cdot x = e 
  \]
  where $y$ is called the inverse of $x$, denoted as $x^{-1}$.
\end{definition}

\begin{proposition}[Uniqueness of Inverse]{uniqueness_of_inverse}
  Let $(S,\cdot)$ be a monoid. If $x \in S$ is invertible, then its inverse is unique. That is, if $y,y' \in S$ are both inverses of $x$, then $y = y'$.
\end{proposition}

\begin{proof}
  Let $y$ and $y'$ be inverses of $x$. Then:
  \begin{align*}
    y &= y \cdot e \\
    &= y \cdot (x \cdot y') \\
    &= (y \cdot x) \cdot y' \\
    &= e \cdot y' \\
    &= y'
  \end{align*}
  Therefore, the inverse is unique.
\end{proof}

\begin{definition}[Group]{group}
  Let $(G,\cdot)$ be a monoid. We say it is a group if every element in $G$ is invertible. 
  
  Equivalently, if $\cdot$ is a binary operation on $G$, we say $(G,\cdot)$ is a group, or $G$ forms a group under $\cdot$, when this operation satisfies:
  \begin{enumerate}
    \item Associativity: For all $x,y,z \in G$
    \[
      x \cdot (y \cdot z) = (x \cdot y) \cdot z 
    \]
    
    \item Identity element: There exists $e \in G$ such that for all $x \in G$
    \[
      x \cdot e = e \cdot x = x 
    \]
    
    \item Inverse elements: For each $x \in G$, there exists $y \in G$ such that
    \[
      x \cdot y = y \cdot x = e 
    \]
  \end{enumerate}
\end{definition}

\begin{proposition}
  Let $(G,\cdot)$ be a group and $x \in G$. Then $(x^{-1})^{-1} = x$.
\end{proposition}

\begin{proof}
  Let $y = x^{-1}$. Then:
  \[
    y \cdot x = x \cdot y = e
  \]
  This shows that $x$ is the inverse of $y = x^{-1}$.
  Therefore, $(x^{-1})^{-1} = x$.
\end{proof}

\begin{proposition}[Inverse of Product]
  Let $(G,\cdot)$ be a group and $x,y \in G$. Then $(x \cdot y)^{-1} = y^{-1} \cdot x^{-1}$.
\end{proposition}

\begin{proof}
  We will show that $y^{-1} \cdot x^{-1}$ is the inverse of $x \cdot y$:
  \begin{align*}
    (x \cdot y)(y^{-1} \cdot x^{-1}) &= x \cdot (y \cdot y^{-1}) \cdot x^{-1} \\
    &= x \cdot e \cdot x^{-1} \\
    &= x \cdot x^{-1} \\
    &= e
  \end{align*}
  Similarly, $(y^{-1} \cdot x^{-1})(x \cdot y) = e$.
  Therefore, $(x \cdot y)^{-1} = y^{-1} \cdot x^{-1}$.
\end{proof}

\begin{definition}[Abelian Group]{abelian_group}
  Let $(G,\cdot)$ be a group. We say it is an abelian group, or commutative group, if the operation satisfies the commutative law:
  \[
    \forall x,y \in G, \quad x \cdot y = y \cdot x
  \]
\end{definition}

\begin{lemma}
  Let $(S,\cdot)$ be a monoid and let $G$ be the subset of all invertible elements in $S$. Then $(G,\cdot)$ is a group.
\end{lemma}

\begin{proof}
  We need to verify three group axioms:
  \begin{enumerate}
    \item Closure: If $x,y \in G$, then $x \cdot y \in G$ (as product of invertible elements is invertible)
    \item Identity: $e \in G$ (as $e$ is invertible)
    \item Inverse: If $x \in G$, then $x^{-1} \in G$ (by definition of invertible elements)
  \end{enumerate}
  Associativity is inherited from $S$. Therefore, $(G,\cdot)$ is a group.
\end{proof}

\begin{definition}[General Linear Group]{general_linear_group}
  The group of $n \times n$ invertible real matrices under matrix multiplication is called the general linear group of degree $n$ over the real numbers, denoted as $(GL(n,\mathbb{R}),\cdot)$. Since a matrix is invertible if and only if its determinant is nonzero:
  \[
    GL(n,\mathbb{R}) = \{ A \in M(n,\mathbb{R}) : \det(A) \neq 0 \}
  \]
\end{definition}

\begin{definition}[Special Linear Group]
  The special linear group of degree $n$ over the real numbers is the group of $n \times n$ real matrices with determinant exactly $1$ under matrix multiplication, denoted as $(SL(n,\mathbb{R}),\cdot)$. That is:
  \[
    SL(n,\mathbb{R}) = \{ A \in M(n,\mathbb{R}) : \det(A) = 1 \}
  \]
\end{definition}

\begin{definition}[Subgroup]{subgroup}
  Let $(G,\cdot)$ be a group and $H \subset G$. We say $H$ is a subgroup of $G$, denoted as $H < G$, if it contains the identity element and is closed under multiplication and inverse operations. That is:
  \begin{enumerate}
    \item $\forall x,y \in H, \quad x \cdot y \in H$ \quad (closure under multiplication)
    \item $\forall x \in H, \quad x^{-1} \in H$ \quad (closure under inverse)
    \item $e \in H$ \quad (contains identity)
  \end{enumerate}
\end{definition}

\begin{proposition}
  Let $(G,\cdot)$ be a group. If $H$ is a subgroup of $G$, then $(H,\cdot)$ is also a group.
\end{proposition}

\begin{proof}
  Since $H$ is a subgroup:
  \begin{enumerate}
    \item Associativity: Inherited from $G$
    \item Identity: $e \in H$ by definition of subgroup
    \item Inverse: For all $x \in H$, $x^{-1} \in H$ by definition of subgroup
    \item Closure: For all $x,y \in H$, $x \cdot y \in H$ by definition of subgroup
  \end{enumerate}
  Therefore, $(H,\cdot)$ satisfies all group axioms.
\end{proof}

\begin{proposition}
  For convenience, we can combine the first two conditions of a subgroup definition~\ref{def:subgroup} into one, reducing to two conditions:
  \begin{enumerate}
    \item $\forall x,y \in H, \quad x \cdot y^{-1} \in H$ 
    \item $e \in H$ 
  \end{enumerate}
  These conditions are equivalent to the original subgroup definition.
\end{proposition}

\begin{proof}

  ($\Rightarrow$)  $\forall y \in H$, $y^{-1} \in H$, then the closure under multiplication,  $\forall x,y,y^{-1} \in H$, $x \cdot y^{-1} \in H$
  
  ($\Leftarrow$) $\forall x,y \in H$, $x \cdot y^{-1} \in H$, let $x=e$, then have $\forall y\in H$, $y^{-1} \in H$; so $\forall x,y^{-1}\in H$, $x \cdot (y^{-1})^{-1} \in H$, then $x \cdot y \in H$.
\end{proof}

\begin{proposition}
  $(SL(n,\mathbb{R}),\cdot)$ is a group.
\end{proposition}

\begin{proof}
  We verify the group axioms:
  \begin{enumerate}
    \item Closure: If $A,B \in SL(n,\mathbb{R})$, then $\det(AB) = \det(A)\det(B) = 1 \cdot 1 = 1$, so $AB \in SL(n,\mathbb{R})$
    
    \item Identity: The identity matrix $I_n \in SL(n,\mathbb{R})$ since $\det(I_n) = 1$
    
    \item Inverse: If $A \in SL(n,\mathbb{R})$, then $\det(A^{-1}) = \frac{1}{\det(A)} = 1$, so $A^{-1} \in SL(n,\mathbb{R})$
    
    \item Associativity: Inherited from matrix multiplication
  \end{enumerate}
  Therefore, $(SL(n,\mathbb{R}),\cdot)$ is a group.
\end{proof}

\begin{definition}[Group Homomorphism]
  Let $(G,\cdot)$ and $(G',\ast)$ be groups, and let $f : G \to G'$ be a mapping. We say $f$ is a group homomorphism if it preserves the operation, that is:
  \[
    \forall x,y \in G, \quad f(x \cdot y) = f(x) \ast f(y)
  \]
\end{definition}

\begin{proposition}
  Let $f : (G,\cdot) \to (G',\ast)$ be a group homomorphism. Then:
  \begin{enumerate}
    \item $f(e) = e'$ (preserves identity)
    \item $f(x^{-1}) = f(x)^{-1}$ (preserves inverses)
  \end{enumerate}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item For identity element:
    \begin{align*}
      f(e) \ast f(e) &= f(e \cdot e) = f(e) \quad \text{left multiply by $f(e)^{-1}$} \\
      \therefore f(e) &= e'
    \end{align*}
    \item For inverse elements:
    \begin{align*}
      f(x) \ast f(x^{-1}) &= f(x \cdot x^{-1}) = f(e) = e' \quad \text{left multiply by $f(x)^{-1}$} \\
      \therefore f(x^{-1}) &= f(x)^{-1}
    \end{align*}
  \end{enumerate}
\end{proof}



%\problemset

% \printbibliography[heading=bibintoc, title=\ebibname]
% \appendix


% \chapter{Mathematical Tools}

% This appendix covers some of the basic mathematics used in econometrics. We briefly discuss the properties of summation operators, study the properties of linear and some nonlinear equations, and  also introduce some special functions that are common in econometrics applications, including quadratic functions and natural logarithms. The first four sections require only basic algebraic techniques. The fifth section briefly reviews differential Calculus Although Calculus is not necessary to understand much of this book, it is used in some of the end-of-chapter appendices and in some of the more advanced topics in part 3.

% \section{Summation Operator and Description Statistics}

% \textbf{Summation Operator} is an abbreviation used to express the summation of numbers, it plays an important role in statistics and econometrics analysis. If $\{x_i: i=1, 2, \ldots, n\}$ is a sequence of $n$ numbers, the summation of the $n$ numbers is:




\end{document}